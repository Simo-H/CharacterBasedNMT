{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvISaD9HViJl",
    "outputId": "b26c064b-9c06-4454-c65a-717b8ef61d90",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:50.941789Z",
     "start_time": "2024-09-23T12:54:50.936136Z"
    }
   },
   "source": [
    "# !unzip dataset.zip"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "source": [
    "# !unzip charactertokenizer.zip"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9Sr-uJ4ee6s",
    "outputId": "f38f42b9-0a37-42fc-9483-c83cad2c84b6",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:50.985803Z",
     "start_time": "2024-09-23T12:54:50.980235Z"
    }
   },
   "outputs": [],
   "execution_count": 69
  },
  {
   "cell_type": "code",
   "source": [
    "!nvidia-smi"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6hriyQCo_OE",
    "outputId": "74d3b43d-dab2-4ee8-bc70-c8b49954b2d6",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:51.156598Z",
     "start_time": "2024-09-23T12:54:51.043265Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Sep 23 15:54:51 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 551.61                 Driver Version: 551.61         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 4000 with Max...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   53C    P8             15W /   50W |    5333MiB /   8192MiB |      1%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1808    C+G   ...CA Client\\Browser\\CtxWebBrowser.exe      N/A      |\n",
      "|    0   N/A  N/A      3908    C+G   ...t\\SelfServicePlugin\\SelfService.exe      N/A      |\n",
      "|    0   N/A  N/A      9152    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
      "|    0   N/A  N/A     13192    C+G   ...on\\128.0.2739.79\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     13732    C+G   ...on\\128.0.2739.79\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     15444    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     16136    C+G   ...2txyewy\\StartMenuExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     16204    C+G   ...aam7r\\AcrobatNotificationClient.exe      N/A      |\n",
      "|    0   N/A  N/A     18196    C+G   ...\\Docker\\frontend\\Docker Desktop.exe      N/A      |\n",
      "|    0   N/A  N/A     18868      C   ...rograms\\Python\\Python312\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     24760    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe      N/A      |\n",
      "|    0   N/A  N/A     26152    C+G   ...6.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
      "|    0   N/A  N/A     28088    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
      "|    0   N/A  N/A     29068    C+G   ...ns\\PyCharm 2023.1\\bin\\pycharm64.exe      N/A      |\n",
      "|    0   N/A  N/A     31788    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
      "|    0   N/A  N/A     33428    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
      "|    0   N/A  N/A     35676    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
      "|    0   N/A  N/A     37372    C+G   ...on\\128.0.2739.79\\msedgewebview2.exe      N/A      |\n",
      "|    0   N/A  N/A     37764    C+G   ...573_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     38312      C   ...rograms\\Python\\Python312\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     38324    C+G   ...1.0_x64__8wekyb3d8bbwe\\Video.UI.exe      N/A      |\n",
      "|    0   N/A  N/A     40532    C+G   ...573_x64__8wekyb3d8bbwe\\ms-teams.exe      N/A      |\n",
      "|    0   N/A  N/A     42528    C+G   ....5635.0_x64__8j3eq9eme6ctt\\IGCC.exe      N/A      |\n",
      "|    0   N/A  N/A     43364    C+G   ...8bbwe\\SnippingTool\\SnippingTool.exe      N/A      |\n",
      "|    0   N/A  N/A     44960    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "pG4HVIt1pMzy",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:51.161593Z",
     "start_time": "2024-09-23T12:54:51.156598Z"
    }
   },
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrJWaRSaWGb9",
    "outputId": "3e792dc3-832d-4c5d-b5d1-11ca91dfef8c",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:51.167223Z",
     "start_time": "2024-09-23T12:54:51.161593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "source": [
    "# create a torch dataset from the html and json files\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from charactertokenizer import CharacterTokenizer"
   ],
   "metadata": {
    "id": "SRXukzbGWWxE",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:51.171659Z",
     "start_time": "2024-09-23T12:54:51.167223Z"
    }
   },
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_attrs(soup):\n",
    "    for tag in soup.find_all(True):\n",
    "        tag.attrs = {}\n",
    "    return soup\n",
    "\n",
    "# removing trailing and leading whitespaces from tag.strings for all html data\n",
    "def remove_whitespace(soup):\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.string is None:\n",
    "            continue\n",
    "        tag.string = tag.string.strip()\n",
    "    return soup"
   ],
   "metadata": {
    "id": "EhqvsroaWrOe",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:51.186161Z",
     "start_time": "2024-09-23T12:54:51.180181Z"
    }
   },
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "source": [
    "# load all html a store it in memory to save time in io operations\n",
    "html_data = []\n",
    "html_str_data = []\n",
    "for i in range(len(os.listdir('./generated_tables/tables'))):\n",
    "    if i == 1000:\n",
    "        break\n",
    "    with open(f'./generated_tables/tables/{i}_table.html') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        soup = remove_attrs(soup)\n",
    "        soup = remove_whitespace(soup)\n",
    "        html_data.append(soup)\n",
    "        #### TODO: remove the newlines between tags in the html files but not from the string data, e.g. from the soup object.\n",
    "        html_str_data.append(str(soup).replace(\">\\n<\", \"><\"))"
   ],
   "metadata": {
    "id": "BlVl6Mw-W0FM",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:53.808063Z",
     "start_time": "2024-09-23T12:54:51.227430Z"
    }
   },
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "source": [
    "# building a tokenizer for the html data, each tag is a token and the characters inside the tags are also tokens\n",
    "# get a set of all tags in the html files\n",
    "html_regular_tokens = set()\n",
    "html_special_tokens = set()\n",
    "for html_file in html_data:\n",
    "    # add all tags to the set\n",
    "    for tag in html_file.find_all(True):\n",
    "        html_special_tokens.add(\"<{tag_name}>\".format(tag_name = tag.name))\n",
    "        html_special_tokens.add(\"</{tag_name}>\".format(tag_name = tag.name))\n",
    "    # add all characters to the set\n",
    "    for char in html_file.get_text():\n",
    "        html_regular_tokens.add(char)"
   ],
   "metadata": {
    "id": "0FllYATuW8-E",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:53.920814Z",
     "start_time": "2024-09-23T12:54:53.808063Z"
    }
   },
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "source": [
    "# create a tokenizer for the html data\n",
    "html_tokenizer = CharacterTokenizer(html_regular_tokens, html_special_tokens, 1000, padding=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcVJITOAXJti",
    "outputId": "cebb1784-3b7e-49fe-d493-18ed667ddb27",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:53.925798Z",
     "start_time": "2024-09-23T12:54:53.920814Z"
    }
   },
   "outputs": [],
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def encode(self, obj):\n",
    "        def custom_format(value):\n",
    "            if isinstance(value, dict):\n",
    "                items = [f'[{json.dumps(k)}][:]{custom_format(v)}' for k, v in value.items()]\n",
    "                return f'[{{]{\"[,]\".join(items)}[}}]'\n",
    "            elif isinstance(value, list):\n",
    "                items = [custom_format(v) for v in value]\n",
    "                return f'[[]{\"[,]\".join(items)}[]]'\n",
    "            else:\n",
    "                return json.dumps(value)\n",
    "\n",
    "        return custom_format(obj)"
   ],
   "metadata": {
    "id": "7IxvS-0FZcZE",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:53.932838Z",
     "start_time": "2024-09-23T12:54:53.925798Z"
    }
   },
   "outputs": [],
   "execution_count": 78
  },
  {
   "cell_type": "code",
   "source": [
    "json_data = []\n",
    "json_str_data = []\n",
    "for i in range(len(os.listdir('./generated_tables/metadata'))):\n",
    "    if i == 1000:\n",
    "      break\n",
    "    with open(f'./generated_tables/metadata/{i}_metadata.json') as f:\n",
    "        parsed_json = json.load(f)\n",
    "        json_data.append(parsed_json)\n",
    "        json_str_data.append(json.dumps(parsed_json, cls=CustomJSONEncoder))"
   ],
   "metadata": {
    "id": "E0AxJ3KdZjle",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.132979Z",
     "start_time": "2024-09-23T12:54:53.932838Z"
    }
   },
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "source": [
    "html_data[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbhlqm9JK5PX",
    "outputId": "75fb7c45-db7d-48c8-c28a-a98767f14cb5",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.141685Z",
     "start_time": "2024-09-23T12:54:54.135415Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table>\n",
       "<caption>Table 59.99.9.62 Loss adjuster, chartered</caption>\n",
       "<thead>\n",
       "<tr>\n",
       "<th></th>\n",
       "<th>Daniel Brown</th>\n",
       "<th>Shane Barnes DDS</th>\n",
       "<th>Nicole Carpenter</th>\n",
       "<th>Kristin Duarte</th>\n",
       "</tr>\n",
       "<th></th>\n",
       "<th>programmer</th>\n",
       "<th>Carpenter</th>\n",
       "<th>singer</th>\n",
       "<th>actor</th>\n",
       "\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>Roberts LLC</td>\n",
       "<td>1060</td>\n",
       "<td>37</td>\n",
       "<td>1593</td>\n",
       "<td>1364</td>\n",
       "</tr>\n",
       "</tbody><tfoot>modified: 5Feb2013</tfoot>\n",
       "<tfoot>Creation: 3Feb2013 Chad</tfoot>\n",
       "</table>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "source": [
    "json_data[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPuG5Za7KXn2",
    "outputId": "ac9ad620-0af8-4f15-ccbc-f1ed61f60960",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.146757Z",
     "start_time": "2024-09-23T12:54:54.141685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': {'content': ['1060', '37', '1593', '1364'],\n",
       "  'headers': {'col': ['Roberts LLC'],\n",
       "   'row': ['Daniel Brown',\n",
       "    'Shane Barnes DDS',\n",
       "    'Nicole Carpenter',\n",
       "    'Kristin Duarte',\n",
       "    'programmer',\n",
       "    'Carpenter',\n",
       "    'singer',\n",
       "    'actor']}},\n",
       " 'footer': {'table_creation_date:': '3Feb2013',\n",
       "  'text': 'modified: 5Feb2013\\nCreation: 3Feb2013 Chad'},\n",
       " 'header': {'table_id': '59.99.9.62',\n",
       "  'text': 'Table 59.99.9.62 Loss adjuster, chartered'}}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "source": [
    "def get_keys(dictionary):\n",
    "    keys = set()\n",
    "    if isinstance(dictionary, list):\n",
    "        for item in dictionary:\n",
    "            keys.update(get_keys(item))\n",
    "    elif isinstance(dictionary, dict):\n",
    "        for key in dictionary:\n",
    "            keys.add(key)\n",
    "            keys.update(get_keys(dictionary[key]))\n",
    "    return keys"
   ],
   "metadata": {
    "id": "J2XNcIp3Zo2-",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.151181Z",
     "start_time": "2024-09-23T12:54:54.147762Z"
    }
   },
   "outputs": [],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "source": [
    "def get_values(dictionary):\n",
    "    values = set()\n",
    "    if isinstance(dictionary, list):\n",
    "        for item in dictionary:\n",
    "            values.update(get_values(item))\n",
    "    elif isinstance(dictionary, dict):\n",
    "        for key, value in dictionary.items():\n",
    "            # values.add(value)\n",
    "            values.update(get_values(dictionary[key]))\n",
    "    else:\n",
    "        for value in dictionary:\n",
    "            values.add(value)\n",
    "    return values"
   ],
   "metadata": {
    "id": "enVS5XaqZsDa",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.155530Z",
     "start_time": "2024-09-23T12:54:54.152188Z"
    }
   },
   "outputs": [],
   "execution_count": 83
  },
  {
   "cell_type": "code",
   "source": [
    "# building a tokenizer for the html data, each tag is a token and the characters inside the tags are also tokens\n",
    "# get a set of all tags in the html files\n",
    "json_regular_tokens = set()\n",
    "json_special_tokens = set()\n",
    "json_special_tokens.add(\"[{]\")\n",
    "json_special_tokens.add(\"[}]\")\n",
    "json_special_tokens.add(\"[:]\")\n",
    "json_special_tokens.add(\"[,]\")\n",
    "json_special_tokens.add(\"[[]\")\n",
    "json_special_tokens.add(\"[]]\")\n",
    "json_regular_tokens.add(\"\\\"\")\n",
    "json_regular_tokens.add(\"\\\\\")\n",
    "for json_file in json_data:\n",
    "    # add all tags to the set\n",
    "    for key in get_keys(json_file):\n",
    "        json_special_tokens.add(f\"[\\\"{key}\\\"]\")\n",
    "    # add all characters to the set\n",
    "    json_regular_tokens.update(get_values(json_file))"
   ],
   "metadata": {
    "id": "95zEwRD3ZuM-",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.211440Z",
     "start_time": "2024-09-23T12:54:54.157539Z"
    }
   },
   "outputs": [],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "source": [
    "# create a tokenizer for the json data\n",
    "json_tokenizer = CharacterTokenizer(json_regular_tokens, json_special_tokens, 1000)"
   ],
   "metadata": {
    "id": "OniYfJsLaRhH",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.217218Z",
     "start_time": "2024-09-23T12:54:54.211440Z"
    }
   },
   "outputs": [],
   "execution_count": 85
  },
  {
   "cell_type": "code",
   "source": [
    "class BeaconCureDataset(Dataset):\n",
    "    def __init__(self, html_data, json_data, html_tokenizer, json_tokenizer):\n",
    "        self.html_data = [html_tokenizer.encode(html_str) for html_str in html_data]\n",
    "        self.json_data = [json_tokenizer.encode(json_str) for json_str in json_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.html_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.html_data[idx]), torch.LongTensor(self.json_data[idx])"
   ],
   "metadata": {
    "id": "DE7Gaa39aT_V",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.222500Z",
     "start_time": "2024-09-23T12:54:54.217218Z"
    }
   },
   "outputs": [],
   "execution_count": 86
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch, PAD_TOKEN_HTML, PAD_TOKEN_JSON):\n",
    "    src_batch, tgt_batch = list(zip(*batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_TOKEN_HTML)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_TOKEN_JSON)\n",
    "    return src_batch, tgt_batch"
   ],
   "metadata": {
    "id": "VB_p8coEaXJV",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:54.226573Z",
     "start_time": "2024-09-23T12:54:54.222500Z"
    }
   },
   "outputs": [],
   "execution_count": 87
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "PAD_TOKEN_HTML = html_tokenizer.get_vocab()['[PAD]']\n",
    "PAD_TOKEN_JSON = json_tokenizer.get_vocab()['[PAD]']\n",
    "\n",
    "collate_fn_partial = partial(collate_fn, PAD_TOKEN_HTML = PAD_TOKEN_HTML, PAD_TOKEN_JSON = PAD_TOKEN_JSON)\n",
    "bc_dataset = BeaconCureDataset(html_str_data, json_str_data, html_tokenizer, json_tokenizer)\n",
    "# train_dataloader = DataLoader(bc_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n"
   ],
   "metadata": {
    "id": "jah7amTuaYm2",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.133725Z",
     "start_time": "2024-09-23T12:54:54.226573Z"
    }
   },
   "outputs": [],
   "execution_count": 88
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ],
   "metadata": {
    "id": "Ysjv_W-ykiCS",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.138478Z",
     "start_time": "2024-09-23T12:54:55.133725Z"
    }
   },
   "outputs": [],
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ],
   "metadata": {
    "id": "jz6k6RH0abCe",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.149104Z",
     "start_time": "2024-09-23T12:54:55.139488Z"
    }
   },
   "outputs": [],
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    print(\"mask.shape:\" + str(mask.shape))\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    # tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    # print(\"tgt_mask.shape:\" + str(tgt_mask.shape))\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "    # print(\"src_mask.shape:\" + str(src_mask.shape))\n",
    "    src_padding_mask = (src == PAD_TOKEN_HTML).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_TOKEN_JSON).transpose(0, 1)\n",
    "    # print(\"src_padding_mask.shape:\" + str(src_padding_mask.shape))\n",
    "    # print(\"tgt_padding_mask.shape:\" + str(tgt_padding_mask.shape))\n",
    "    return src_mask, src_padding_mask, tgt_padding_mask"
   ],
   "metadata": {
    "id": "EqdTUPERaene",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.154677Z",
     "start_time": "2024-09-23T12:54:55.149104Z"
    }
   },
   "outputs": [],
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "source": [
    "# EMB_SIZE = 128\n",
    "# NHEAD = 16\n",
    "# FFN_HID_DIM = 4096\n",
    "# BATCH_SIZE = 32\n",
    "# NUM_ENCODER_LAYERS = 1\n",
    "# NUM_DECODER_LAYERS = 1\n",
    "# lr=0.001"
   ],
   "metadata": {
    "id": "zv51yDKmjBSC",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.158489Z",
     "start_time": "2024-09-23T12:54:55.154677Z"
    }
   },
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.162657Z",
     "start_time": "2024-09-23T12:54:55.158489Z"
    }
   },
   "cell_type": "code",
   "source": "len(bc_dataset)\n",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(html_tokenizer)\n",
    "TGT_VOCAB_SIZE = len(json_tokenizer)\n",
    "EMB_SIZE = 32\n",
    "NHEAD = 4\n",
    "FFN_HID_DIM = 256\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "LR = 0.001\n",
    "torch.cuda.empty_cache()\n",
    "# train_set, val_set = torch.utils.data.random_split(bc_dataset, [24000, 6000])\n",
    "train_samples_num = int(len(bc_dataset) * 0.8)\n",
    "train_set, val_set = torch.utils.data.random_split(bc_dataset, [train_samples_num, len(bc_dataset) - train_samples_num])\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n",
    "validation_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_JSON)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=False)"
   ],
   "metadata": {
    "id": "EGPBZRMCagG1",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.364660Z",
     "start_time": "2024-09-23T12:54:55.162657Z"
    }
   },
   "outputs": [],
   "execution_count": 94
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for i, (src, tgt) in enumerate(train_dataloader):\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(DEVICE)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # reduce lr on plateau\n",
    "        \n",
    "        # print(\"Batch: {0}, Loss: {1}\".format(i, loss.detach().item()))\n",
    "        losses += loss.detach().item()\n",
    "        del src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, logits, tgt_input, tgt_out, loss\n",
    "        torch.cuda.empty_cache()\n",
    "    return losses / len(list(train_dataloader))"
   ],
   "metadata": {
    "id": "ZiO9ZTqRahsp",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.371457Z",
     "start_time": "2024-09-23T12:54:55.364660Z"
    }
   },
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T12:54:55.377420Z",
     "start_time": "2024-09-23T12:54:55.371457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in validation_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(DEVICE)\n",
    "        \n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(validation_dataloader))"
   ],
   "outputs": [],
   "execution_count": 96
  },
  {
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 1000\n",
    "# with profile(activities=[\n",
    "        # ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, profile_memory=True) as prof:\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    transformer.train()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    scheduler.step(train_loss)\n",
    "    # evaluation\n",
    "    transformer.eval()\n",
    "    val_loss = evaluate(transformer)\n",
    "    # add save model checkpoint every 20 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': transformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "                }, f\"./checkpoints/checkpoint_{epoch}.pt\")\n",
    "    # val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "# save the model after training\n",
    "torch.save(transformer.state_dict(), \"./models/transformer.pt\")\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=30))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZFy7k90ai8e",
    "outputId": "5a440986-0e16-4c1a-bcbc-fbf85b450c39",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:55:04.678698Z",
     "start_time": "2024-09-23T12:54:55.377420Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[97], line 13\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# evaluation\u001B[39;00m\n\u001B[0;32m     12\u001B[0m transformer\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m---> 13\u001B[0m val_loss \u001B[38;5;241m=\u001B[39m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# add save model checkpoint every 20 epochs\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "Cell \u001B[1;32mIn[96], line 14\u001B[0m, in \u001B[0;36mevaluate\u001B[1;34m(model)\u001B[0m\n\u001B[0;32m     11\u001B[0m src_mask, src_padding_mask, tgt_padding_mask \u001B[38;5;241m=\u001B[39m create_mask(src, tgt_input)\n\u001B[0;32m     12\u001B[0m tgt_mask \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtransformer\u001B[38;5;241m.\u001B[39mgenerate_square_subsequent_mask(tgt_input\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m))\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m---> 14\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43msrc_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m tgt_out \u001B[38;5;241m=\u001B[39m tgt[\u001B[38;5;241m1\u001B[39m:, :]\n\u001B[0;32m     17\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), tgt_out\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[90], line 72\u001B[0m, in \u001B[0;36mSeq2SeqTransformer.forward\u001B[1;34m(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\u001B[0m\n\u001B[0;32m     70\u001B[0m src_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_encoding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msrc_tok_emb(src))\n\u001B[0;32m     71\u001B[0m tgt_emb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpositional_encoding(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtgt_tok_emb(trg))\n\u001B[1;32m---> 72\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_emb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m                        \u001B[49m\u001B[43msrc_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerator(outs)\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:220\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[1;34m(self, src, tgt, src_mask, tgt_mask, memory_mask, src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask, src_is_causal, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe feature number of src and tgt must be equal to d_model\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    218\u001B[0m memory \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(src, mask\u001B[38;5;241m=\u001B[39msrc_mask, src_key_padding_mask\u001B[38;5;241m=\u001B[39msrc_key_padding_mask,\n\u001B[0;32m    219\u001B[0m                       is_causal\u001B[38;5;241m=\u001B[39msrc_is_causal)\n\u001B[1;32m--> 220\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    221\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    222\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    223\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:492\u001B[0m, in \u001B[0;36mTransformerDecoder.forward\u001B[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[0;32m    489\u001B[0m output \u001B[38;5;241m=\u001B[39m tgt\n\u001B[0;32m    491\u001B[0m seq_len \u001B[38;5;241m=\u001B[39m _get_seq_len(tgt, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mself_attn\u001B[38;5;241m.\u001B[39mbatch_first)\n\u001B[1;32m--> 492\u001B[0m tgt_is_causal \u001B[38;5;241m=\u001B[39m \u001B[43m_detect_is_causal_mask\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mseq_len\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m    495\u001B[0m     output \u001B[38;5;241m=\u001B[39m mod(output, memory, tgt_mask\u001B[38;5;241m=\u001B[39mtgt_mask,\n\u001B[0;32m    496\u001B[0m                  memory_mask\u001B[38;5;241m=\u001B[39mmemory_mask,\n\u001B[0;32m    497\u001B[0m                  tgt_key_padding_mask\u001B[38;5;241m=\u001B[39mtgt_key_padding_mask,\n\u001B[0;32m    498\u001B[0m                  memory_key_padding_mask\u001B[38;5;241m=\u001B[39mmemory_key_padding_mask,\n\u001B[0;32m    499\u001B[0m                  tgt_is_causal\u001B[38;5;241m=\u001B[39mtgt_is_causal,\n\u001B[0;32m    500\u001B[0m                  memory_is_causal\u001B[38;5;241m=\u001B[39mmemory_is_causal)\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:971\u001B[0m, in \u001B[0;36m_detect_is_causal_mask\u001B[1;34m(mask, is_causal, size)\u001B[0m\n\u001B[0;32m    968\u001B[0m \u001B[38;5;66;03m# Do not use `torch.equal` so we handle batched masks by\u001B[39;00m\n\u001B[0;32m    969\u001B[0m \u001B[38;5;66;03m# broadcasting the comparison.\u001B[39;00m\n\u001B[0;32m    970\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39msize() \u001B[38;5;241m==\u001B[39m causal_comparison\u001B[38;5;241m.\u001B[39msize():\n\u001B[1;32m--> 971\u001B[0m     make_causal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcausal_comparison\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mall\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    972\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    973\u001B[0m     make_causal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "cell_type": "code",
   "source": [
    "# TO-DO\n",
    "# add save model\n",
    "# add checkpoint\n",
    "# add reduce lr on platau\n",
    "# add split train and validation\n",
    "# add seed\n",
    "# add nice train print\n",
    "# add evaluation\n",
    "# "
   ],
   "metadata": {
    "id": "2z8o7p9xcZ-P"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
