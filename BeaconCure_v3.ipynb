{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "rvISaD9HViJl",
    "ExecuteTime": {
     "end_time": "2024-09-23T13:52:28.462098Z",
     "start_time": "2024-09-23T13:52:28.446664Z"
    }
   },
   "source": [
    "# !unzip dataset.zip"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "source": [
    "# !unzip charactertokenizer.zip"
   ],
   "metadata": {
    "id": "V9Sr-uJ4ee6s",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:56:29.883872Z",
     "start_time": "2024-09-23T12:56:29.874561Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "# !mkdir checkpoints"
   ],
   "metadata": {
    "id": "l_C4etkYeUir",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:56:29.977065Z",
     "start_time": "2024-09-23T12:56:29.971703Z"
    }
   },
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "# !mkdir models"
   ],
   "metadata": {
    "id": "T16TelFoBKTM",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:56:30.002448Z",
     "start_time": "2024-09-23T12:56:29.999290Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "# !nvidia-smi",
   "metadata": {
    "id": "C6hriyQCo_OE",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:56:30.085584Z",
     "start_time": "2024-09-23T12:56:30.080702Z"
    }
   },
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "id": "pG4HVIt1pMzy",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:56:30.098570Z",
     "start_time": "2024-09-23T12:56:30.090183Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrJWaRSaWGb9",
    "outputId": "f943f93c-9ea2-437c-e220-e961fc23d561",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:56:30.127576Z",
     "start_time": "2024-09-23T12:56:30.121746Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "# create a torch dataset from the html and json files\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from charactertokenizer import CharacterTokenizer"
   ],
   "metadata": {
    "id": "SRXukzbGWWxE",
    "ExecuteTime": {
     "end_time": "2024-09-23T15:11:01.733624Z",
     "start_time": "2024-09-23T15:11:01.716758Z"
    }
   },
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "code",
   "source": [
    "def remove_attrs(soup):\n",
    "    for tag in soup.find_all(True):\n",
    "        tag.attrs = {}\n",
    "    return soup\n",
    "\n",
    "# removing trailing and leading whitespaces from tag.strings for all html data\n",
    "def remove_whitespace(soup):\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.string is None:\n",
    "            continue\n",
    "        tag.string = tag.string.strip()\n",
    "    return soup"
   ],
   "metadata": {
    "id": "EhqvsroaWrOe",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:56:30.190231Z",
     "start_time": "2024-09-23T12:56:30.185368Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# load all html a store it in memory to save time in io operations\n",
    "html_data = []\n",
    "html_str_data = []\n",
    "for i in range(len(os.listdir('./generated_tables/tables'))):\n",
    "    with open(f'./generated_tables/tables/{i}_table.html') as f:\n",
    "        html = f.read().replace(\">\\n<\", \"><\")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        soup = remove_attrs(soup)\n",
    "        soup = remove_whitespace(soup)\n",
    "        html_data.append(soup)\n",
    "        #### TODO: remove the newlines between tags in the html files but not from the string data, e.g. from the soup object.\n",
    "        html_str_data.append(str(soup))"
   ],
   "metadata": {
    "id": "BlVl6Mw-W0FM",
    "ExecuteTime": {
     "end_time": "2024-09-23T14:11:02.917670Z",
     "start_time": "2024-09-23T14:08:46.814874Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m f \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m>\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m<\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m><\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      7\u001B[0m soup \u001B[38;5;241m=\u001B[39m BeautifulSoup(f, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m soup \u001B[38;5;241m=\u001B[39m remove_attrs(soup)\n\u001B[0;32m      9\u001B[0m soup \u001B[38;5;241m=\u001B[39m remove_whitespace(soup)\n\u001B[0;32m     10\u001B[0m html_data\u001B[38;5;241m.\u001B[39mappend(soup)\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:937\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:928\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\\\pydevd_cython_win32_312_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_312_64.PyDBFrame.do_wait_suspend\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2023.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1201\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1198\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1200\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1201\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2023.1\\plugins\\python\\helpers\\pydev\\pydevd.py:1216\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1213\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1215\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1216\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1220\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "source": [
    "# building a tokenizer for the html data, each tag is a token and the characters inside the tags are also tokens\n",
    "# get a set of all tags in the html files\n",
    "html_regular_tokens = set()\n",
    "html_special_tokens = set()\n",
    "for html_file in html_data:\n",
    "    # add all tags to the set\n",
    "    for tag in html_file.find_all(True):\n",
    "        html_special_tokens.add(\"<{tag_name}>\".format(tag_name = tag.name))\n",
    "        html_special_tokens.add(\"</{tag_name}>\".format(tag_name = tag.name))\n",
    "    # add all characters to the set\n",
    "    for char in html_file.get_text():\n",
    "        html_regular_tokens.add(char)"
   ],
   "metadata": {
    "id": "0FllYATuW8-E",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:39.473885Z",
     "start_time": "2024-09-23T12:57:36.145612Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "# create a tokenizer for the html data\n",
    "html_tokenizer = CharacterTokenizer(html_regular_tokens, html_special_tokens)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FcVJITOAXJti",
    "outputId": "32d809ee-eb65-4470-f841-516b80e799c6",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:39.481067Z",
     "start_time": "2024-09-23T12:57:39.473885Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def encode(self, obj):\n",
    "        def custom_format(value):\n",
    "            if isinstance(value, dict):\n",
    "                items = [f'[{json.dumps(k)}][:]{custom_format(v)}' for k, v in value.items()]\n",
    "                return f'[{{]{\"[,]\".join(items)}[}}]'\n",
    "            elif isinstance(value, list):\n",
    "                items = [custom_format(v) for v in value]\n",
    "                return f'[[]{\"[,]\".join(items)}[]]'\n",
    "            else:\n",
    "                return json.dumps(value)\n",
    "\n",
    "        return custom_format(obj)"
   ],
   "metadata": {
    "id": "7IxvS-0FZcZE",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:39.487628Z",
     "start_time": "2024-09-23T12:57:39.481067Z"
    }
   },
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "json_data = []\n",
    "json_str_data = []\n",
    "for i in range(len(os.listdir('./generated_tables/metadata'))):\n",
    "    with open(f'./generated_tables/metadata/{i}_metadata.json') as f:\n",
    "        parsed_json = json.load(f)\n",
    "        json_data.append(parsed_json)\n",
    "        json_str_data.append(json.dumps(parsed_json, cls=CustomJSONEncoder))"
   ],
   "metadata": {
    "id": "E0AxJ3KdZjle",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:44.412567Z",
     "start_time": "2024-09-23T12:57:39.487628Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "html_data[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbhlqm9JK5PX",
    "outputId": "c3f3a67e-28ee-4c21-8c63-d54cd986d836",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:44.419384Z",
     "start_time": "2024-09-23T12:57:44.412567Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table>\n",
       "<caption>Table 59.99.9.62 Loss adjuster, chartered</caption>\n",
       "<thead>\n",
       "<tr>\n",
       "<th></th>\n",
       "<th>Daniel Brown</th>\n",
       "<th>Shane Barnes DDS</th>\n",
       "<th>Nicole Carpenter</th>\n",
       "<th>Kristin Duarte</th>\n",
       "</tr>\n",
       "<th></th>\n",
       "<th>programmer</th>\n",
       "<th>Carpenter</th>\n",
       "<th>singer</th>\n",
       "<th>actor</th>\n",
       "\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr>\n",
       "<td>Roberts LLC</td>\n",
       "<td>1060</td>\n",
       "<td>37</td>\n",
       "<td>1593</td>\n",
       "<td>1364</td>\n",
       "</tr>\n",
       "</tbody><tfoot>modified: 5Feb2013</tfoot>\n",
       "<tfoot>Creation: 3Feb2013 Chad</tfoot>\n",
       "</table>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "source": [
    "json_data[0]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZPuG5Za7KXn2",
    "outputId": "1928ed5b-3bed-4884-cf8a-36feae253c60",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:44.425356Z",
     "start_time": "2024-09-23T12:57:44.419384Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': {'content': ['1060', '37', '1593', '1364'],\n",
       "  'headers': {'col': ['Roberts LLC'],\n",
       "   'row': ['Daniel Brown',\n",
       "    'Shane Barnes DDS',\n",
       "    'Nicole Carpenter',\n",
       "    'Kristin Duarte',\n",
       "    'programmer',\n",
       "    'Carpenter',\n",
       "    'singer',\n",
       "    'actor']}},\n",
       " 'footer': {'table_creation_date:': '3Feb2013',\n",
       "  'text': 'modified: 5Feb2013\\nCreation: 3Feb2013 Chad'},\n",
       " 'header': {'table_id': '59.99.9.62',\n",
       "  'text': 'Table 59.99.9.62 Loss adjuster, chartered'}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "source": [
    "def get_keys(dictionary):\n",
    "    keys = set()\n",
    "    if isinstance(dictionary, list):\n",
    "        for item in dictionary:\n",
    "            keys.update(get_keys(item))\n",
    "    elif isinstance(dictionary, dict):\n",
    "        for key in dictionary:\n",
    "            keys.add(key)\n",
    "            keys.update(get_keys(dictionary[key]))\n",
    "    return keys"
   ],
   "metadata": {
    "id": "J2XNcIp3Zo2-",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:44.430278Z",
     "start_time": "2024-09-23T12:57:44.425356Z"
    }
   },
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "source": [
    "def get_values(dictionary):\n",
    "    values = set()\n",
    "    if isinstance(dictionary, list):\n",
    "        for item in dictionary:\n",
    "            values.update(get_values(item))\n",
    "    elif isinstance(dictionary, dict):\n",
    "        for key, value in dictionary.items():\n",
    "            # values.add(value)\n",
    "            values.update(get_values(dictionary[key]))\n",
    "    else:\n",
    "        for value in dictionary:\n",
    "            values.add(value)\n",
    "    return values"
   ],
   "metadata": {
    "id": "enVS5XaqZsDa",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:44.434592Z",
     "start_time": "2024-09-23T12:57:44.430278Z"
    }
   },
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "source": [
    "# building a tokenizer for the html data, each tag is a token and the characters inside the tags are also tokens\n",
    "# get a set of all tags in the html files\n",
    "json_regular_tokens = set()\n",
    "json_special_tokens = set()\n",
    "json_special_tokens.add(\"[{]\")\n",
    "json_special_tokens.add(\"[}]\")\n",
    "json_special_tokens.add(\"[:]\")\n",
    "json_special_tokens.add(\"[,]\")\n",
    "json_special_tokens.add(\"[[]\")\n",
    "json_special_tokens.add(\"[]]\")\n",
    "json_regular_tokens.add(\"\\\"\")\n",
    "json_regular_tokens.add(\"\\\\\")\n",
    "for json_file in json_data:\n",
    "    # add all tags to the set\n",
    "    for key in get_keys(json_file):\n",
    "        json_special_tokens.add(f\"[\\\"{key}\\\"]\")\n",
    "    # add all characters to the set\n",
    "    json_regular_tokens.update(get_values(json_file))"
   ],
   "metadata": {
    "id": "95zEwRD3ZuM-",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:45.923223Z",
     "start_time": "2024-09-23T12:57:44.434592Z"
    }
   },
   "outputs": [],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "source": [
    "# create a tokenizer for the json data\n",
    "json_tokenizer = CharacterTokenizer(json_regular_tokens, json_special_tokens)"
   ],
   "metadata": {
    "id": "OniYfJsLaRhH",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:45.928148Z",
     "start_time": "2024-09-23T12:57:45.923223Z"
    }
   },
   "outputs": [],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "source": [
    "class BeaconCureDataset(Dataset):\n",
    "    def __init__(self, html_data, json_data, html_tokenizer, json_tokenizer):\n",
    "        self.html_data = [html_tokenizer.encode(html_str) for html_str in html_data]\n",
    "        self.json_data = [json_tokenizer.encode(json_str) for json_str in json_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.html_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.html_data[idx]), torch.LongTensor(self.json_data[idx])"
   ],
   "metadata": {
    "id": "DE7Gaa39aT_V",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:45.933614Z",
     "start_time": "2024-09-23T12:57:45.929176Z"
    }
   },
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch, PAD_TOKEN_HTML, PAD_TOKEN_JSON):\n",
    "    src_batch, tgt_batch = list(zip(*batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_TOKEN_HTML)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_TOKEN_JSON)\n",
    "    return src_batch, tgt_batch"
   ],
   "metadata": {
    "id": "VB_p8coEaXJV",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:57:45.937320Z",
     "start_time": "2024-09-23T12:57:45.933614Z"
    }
   },
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "# PAD_TOKEN_HTML = html_tokenizer.get_vocab()['[PAD]']\n",
    "# PAD_TOKEN_JSON = json_tokenizer.get_vocab()['[PAD]']\n",
    "PAD_TOKEN_HTML = html_tokenizer.get_vocab()['[MASK]']\n",
    "PAD_TOKEN_JSON = json_tokenizer.get_vocab()['[MASK]']\n",
    "\n",
    "collate_fn_partial = partial(collate_fn, PAD_TOKEN_HTML = PAD_TOKEN_HTML, PAD_TOKEN_JSON = PAD_TOKEN_JSON)\n",
    "bc_dataset = BeaconCureDataset(html_str_data, json_str_data, html_tokenizer, json_tokenizer)\n",
    "# train_dataloader = DataLoader(bc_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n"
   ],
   "metadata": {
    "id": "jah7amTuaYm2",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "44e65fa5-596d-4f66-e8a8-8e42159347d4",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:12.709234Z",
     "start_time": "2024-09-23T12:57:45.937320Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1016 > 1000). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ],
   "metadata": {
    "id": "Ysjv_W-ykiCS",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:12.713907Z",
     "start_time": "2024-09-23T12:58:12.709234Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ],
   "metadata": {
    "id": "jz6k6RH0abCe",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:12.724391Z",
     "start_time": "2024-09-23T12:58:12.713907Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    # tgt_seq_len = tgt.shape[0]\n",
    "    # tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "    src_padding_mask = (src == PAD_TOKEN_HTML).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_TOKEN_JSON).transpose(0, 1)\n",
    "    return src_mask, src_padding_mask, tgt_padding_mask"
   ],
   "metadata": {
    "id": "EqdTUPERaene",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:12.731234Z",
     "start_time": "2024-09-23T12:58:12.724391Z"
    }
   },
   "outputs": [],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "source": [
    "# EMB_SIZE = 256\n",
    "# NHEAD = 8\n",
    "# FFN_HID_DIM = 4096\n",
    "# BATCH_SIZE = 32\n",
    "# NUM_ENCODER_LAYERS = 1\n",
    "# NUM_DECODER_LAYERS = 1\n",
    "# LR = 0.001"
   ],
   "metadata": {
    "id": "zv51yDKmjBSC",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:12.735339Z",
     "start_time": "2024-09-23T12:58:12.731234Z"
    }
   },
   "outputs": [],
   "execution_count": 37
  },
  {
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(html_tokenizer)\n",
    "TGT_VOCAB_SIZE = len(json_tokenizer)\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 4096\n",
    "BATCH_SIZE = 32\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "LR = 0.001\n",
    "torch.cuda.empty_cache()\n",
    "# train_set, val_set = torch.utils.data.random_split(bc_dataset, [24000, 6000])\n",
    "train_samples_num = int(len(bc_dataset) * 0.8)\n",
    "train_set, val_set = torch.utils.data.random_split(bc_dataset, [train_samples_num, len(bc_dataset) - train_samples_num])\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n",
    "validation_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_JSON)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, threshold=0.01, threshold_mode='abs', verbose=True)"
   ],
   "metadata": {
    "id": "EGPBZRMCagG1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "359623ef-a5f2-48fd-c18b-d137eff68482",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:13.646248Z",
     "start_time": "2024-09-23T12:58:12.735339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for i, (src, tgt) in enumerate(train_dataloader):\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(DEVICE)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        # reduce lr on plateau\n",
    "\n",
    "        # print(\"Batch: {0}, Loss: {1}\".format(i, loss.detach().item()))\n",
    "        losses += loss.detach().item()\n",
    "        del src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, logits, tgt_input, tgt_out, loss\n",
    "        torch.cuda.empty_cache()\n",
    "    return losses / len(list(train_dataloader))"
   ],
   "metadata": {
    "id": "ZiO9ZTqRahsp",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:13.651699Z",
     "start_time": "2024-09-23T12:58:13.646248Z"
    }
   },
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "id": "3V-7bHurcNnO",
    "ExecuteTime": {
     "end_time": "2024-09-23T12:58:13.659476Z",
     "start_time": "2024-09-23T12:58:13.651699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    for src, tgt in validation_dataloader:\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(DEVICE)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(validation_dataloader))"
   ],
   "outputs": [],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 20\n",
    "# with profile(activities=[\n",
    "        # ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, profile_memory=True) as prof:\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    transformer.train()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    scheduler.step(train_loss)\n",
    "    # evaluation\n",
    "    transformer.eval()\n",
    "    val_loss = evaluate(transformer)\n",
    "    # add save model checkpoint every 20 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': transformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "                }, f\"./checkpoints/checkpoint_{epoch}.pt\")\n",
    "    # val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "# save the model after training\n",
    "torch.save(transformer.state_dict(), \"./models/transformer.pt\")\n",
    "# print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=30))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 543
    },
    "id": "6ZFy7k90ai8e",
    "outputId": "59d6a694-e653-49a1-9edc-dba5813c5acd",
    "ExecuteTime": {
     "end_time": "2024-09-23T13:32:12.627851Z",
     "start_time": "2024-09-23T12:58:13.659476Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[41], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m      7\u001B[0m transformer\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m----> 8\u001B[0m train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m     10\u001B[0m scheduler\u001B[38;5;241m.\u001B[39mstep(train_loss)\n",
      "Cell \u001B[1;32mIn[39], line 27\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer)\u001B[0m\n\u001B[0;32m     23\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# reduce lr on plateau\u001B[39;00m\n\u001B[0;32m     25\u001B[0m \n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# print(\"Batch: {0}, Loss: {1}\".format(i, loss.detach().item()))\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m losses \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetach\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m src, tgt, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, logits, tgt_input, tgt_out, loss\n\u001B[0;32m     29\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mempty_cache()\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(transformer.state_dict(), \"./models/transformer.pt\")"
   ],
   "metadata": {
    "id": "TLMd_LZvoKas"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.0001"
   ],
   "metadata": {
    "id": "9Ws4gMAKJPJi"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TO-DO\n",
    "# add save model\n",
    "# add checkpoint\n",
    "# add reduce lr on platau\n",
    "# add nice train print"
   ],
   "metadata": {
    "id": "2z8o7p9xcZ-P"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        memory = memory.to(DEVICE)\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, memory, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
    "        if next_word == json_tokenizer.get_vocab()['[SEP]']:\n",
    "            break\n",
    "    return ys"
   ],
   "metadata": {
    "id": "rTIpJusuuWrh"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def translate(model: torch.nn.Module, src_sentence: str):\n",
    "    model.eval()\n",
    "    src = torch.LongTensor(html_tokenizer.encode(src_sentence)).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask, max_len=num_tokens + 100, start_symbol=html_tokenizer.get_vocab()['[CLS]']).flatten()\n",
    "    return \"\".join(json_tokenizer.decode(tgt_tokens.cpu().numpy()))"
   ],
   "metadata": {
    "id": "drXf-bLquu5T"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "json_str_data[-1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "9aQvqt1Ox2FQ",
    "outputId": "6cdaf887-79c2-41be-aa7c-9957ce38a370"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pred = translate(transformer, html_str_data[-1])\n",
    "pred"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "S3Stirn7vjjS",
    "outputId": "8853fe4c-b68e-4341-c752-17f9228eeb4e"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pred[5:-5] == json_str_data[-1]"
   ],
   "metadata": {
    "id": "SX8m2cYJxoQm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5b081e4d-f1c9-48ea-c4fc-1bf04a153a16"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "html_tokenizer.encode(html_str_data[0])"
   ],
   "metadata": {
    "id": "zRI5pfumv9uu"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = torch.load(\"./models/transformer.pt\", weights_only=False)"
   ],
   "metadata": {
    "id": "9RRV8CdWxZ7R"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"./models/transformer.pt\"))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QgMgcXCOxz7i",
    "outputId": "5a749d34-1670-4a9d-d205-b17fc2a9ca16"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pred = translate(model, html_str_data[-1])\n",
    "pred"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "hHBUecdHyRTn",
    "outputId": "8bbea549-2cd0-46fd-b5f0-c799ec6c3b95"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "pred[5:-5] == json_str_data[-1]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9pSvfXEzPAQ",
    "outputId": "8fe0985e-ba0f-43de-86fc-61fe4e3d0855"
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OEYQu-ZlzdFH"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
