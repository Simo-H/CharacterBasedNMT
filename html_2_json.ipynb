{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# !unzip html2json.zip",
   "id": "134eb67770a28698"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install evaluate",
   "id": "de0314824ef63007"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:00:04.061518Z",
     "start_time": "2024-09-27T23:00:04.056087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from functools import partial\n",
    "from html2json import HTML_JSON_Dataset, padding_collate_fn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from html2json.charactertokenizer import HTMLTokenizer, JSONTokenizer\n",
    "from html2json.charactertokenizer import MASK_TOKEN\n",
    "from html2json import load_data\n",
    "from html2json.seq2seq import Seq2SeqTransformer\n",
    "from html2json.seq2seq import translate_greedy_search, translate_beam_search\n",
    "from html2json.training import train_epoch, evaluate\n",
    "from timeit import default_timer as timer\n",
    "from evaluate import load\n",
    "import os"
   ],
   "id": "ff21eea0a43f3d6b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:00:04.936036Z",
     "start_time": "2024-09-27T23:00:04.929260Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.is_available()"
   ],
   "id": "f27c7203cdf39396",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:00:05.374902Z",
     "start_time": "2024-09-27T23:00:05.370427Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')",
   "id": "f38bc17518fbb5d4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:00:05.861304Z",
     "start_time": "2024-09-27T23:00:05.856157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "html_pth = 'generated_tables/tables'\n",
    "json_pth = 'generated_tables/metadata'"
   ],
   "id": "40ec41887aaeb374",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:01:32.746936Z",
     "start_time": "2024-09-27T23:00:07.336769Z"
    }
   },
   "cell_type": "code",
   "source": "html_data, json_data = load_data(html_pth, json_pth, as_string=False, limit=None)",
   "id": "eadff2aae180f018",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:01:35.940360Z",
     "start_time": "2024-09-27T23:01:32.746936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "html_tokenizer = HTMLTokenizer(html_data)\n",
    "json_tokenizer = JSONTokenizer(json_data)"
   ],
   "id": "d740f15ffbbb11a0",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:02:56.434765Z",
     "start_time": "2024-09-27T23:01:35.940691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "collate_fn = partial(padding_collate_fn, pad_token_html = MASK_TOKEN, pad_token_json = MASK_TOKEN)\n",
    "html_data_str, json_data_str = load_data(html_pth, json_pth, as_string=True, limit=None)\n",
    "h2j_dataset = HTML_JSON_Dataset([html_tokenizer.encode(h) for h in html_data_str], [json_tokenizer.encode(j) for j in json_data_str])"
   ],
   "id": "52eb8b200e63227f",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T23:02:56.441876Z",
     "start_time": "2024-09-27T23:02:56.434765Z"
    }
   },
   "cell_type": "code",
   "source": "train_set, val_set = random_split(h2j_dataset, [0.8, 0.2], torch.Generator().manual_seed(42))",
   "id": "853d8261ebf39768",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:30:33.446175Z",
     "start_time": "2024-09-27T21:30:33.437617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(42)\n",
    "SRC_VOCAB_SIZE = len(html_tokenizer)\n",
    "TGT_VOCAB_SIZE = len(json_tokenizer)\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 4096\n",
    "BATCH_SIZE = 32\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS = 1\n",
    "LR = 0.001\n",
    "NUM_EPOCHS = 25\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "validation_dataloader = DataLoader(val_set, batch_size=BATCH_SIZE, collate_fn=collate_fn)"
   ],
   "id": "78977f2811f48ab9",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:30:33.663567Z",
     "start_time": "2024-09-27T21:30:33.447180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "if os.path.exists(\"./assets/transformer.pt\"):\n",
    "    transformer.load_state_dict(torch.load(\"./assets/transformer.pt\", map_location=torch.device(DEVICE)))\n",
    "else:\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "transformer = transformer.to(DEVICE)"
   ],
   "id": "5206e669a57e2f8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "C:\\Users\\Simo\\AppData\\Local\\Temp\\ipykernel_40564\\3811585935.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer.load_state_dict(torch.load(\"./assets/transformer.pt\", map_location=torch.device(DEVICE)))\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=MASK_TOKEN)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=LR, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, threshold=0.1, threshold_mode='rel')"
   ],
   "id": "e2791521a34708e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T20:25:37.919981Z",
     "start_time": "2024-09-23T20:25:30.463761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    transformer.train()\n",
    "    train_loss = train_epoch(transformer, optimizer, train_dataloader, loss_fn)\n",
    "    end_time = timer()\n",
    "    scheduler.step(train_loss)\n",
    "    # evaluation\n",
    "    transformer.eval()\n",
    "    val_loss = evaluate(transformer, validation_dataloader, loss_fn)\n",
    "    # add save model checkpoint every 20 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': transformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': train_loss,\n",
    "                }, f\"./checkpoints/checkpoint_{epoch}.pt\")\n",
    "    # val_loss = evaluate(transformer)\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Train loss: {train_loss:.5f}, Val loss: {val_loss:.5f}, \"f\"Epoch time = {(end_time - start_time):.3f}s, lr: {scheduler.get_last_lr()}\")\n",
    "# save the model after training\n",
    "torch.save(transformer.state_dict(), \"./assets/transformer.pt\")"
   ],
   "id": "ad6623919ae0f9a8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5193: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 4.788, Val loss: 4.303, Epoch time = 7.219s, lr: [0.001]\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T22:59:52.066998Z",
     "start_time": "2024-09-27T22:59:52.049976Z"
    }
   },
   "cell_type": "code",
   "source": "train_idx, val_idx = random_split(range(len(h2j_dataset)), [0.8, 0.2], torch.Generator().manual_seed(42))",
   "id": "16c31b16364b95ce",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'h2j_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[6], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m train_idx, val_idx \u001B[38;5;241m=\u001B[39m random_split(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[43mh2j_dataset\u001B[49m)), [\u001B[38;5;241m0.8\u001B[39m, \u001B[38;5;241m0.2\u001B[39m], torch\u001B[38;5;241m.\u001B[39mGenerator()\u001B[38;5;241m.\u001B[39mmanual_seed(\u001B[38;5;241m42\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'h2j_dataset' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T22:59:47.503713Z",
     "start_time": "2024-09-27T22:59:47.486880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_num = 0\n",
    "idx = val_idx[sample_num]\n",
    "val_idx = html_data_str[idx]"
   ],
   "id": "6e21b4f99e1e6af6",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m sample_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m----> 2\u001B[0m idx \u001B[38;5;241m=\u001B[39m \u001B[43mval_idx\u001B[49m[sample_num]\n\u001B[0;32m      3\u001B[0m val_idx \u001B[38;5;241m=\u001B[39m html_data_str[idx]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'val_idx' is not defined"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:20:09.486409Z",
     "start_time": "2024-09-27T21:20:09.482174Z"
    }
   },
   "cell_type": "code",
   "source": "val_idx",
   "id": "baf7a85ec9d32c09",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><caption>Table 28.13.13.94 Broadcast presenter</caption><thead><tr><th></th><th>James Jones</th><th>Gerald Kelley</th><th>James Potter</th><th>Alexander Hill</th><th>Ryan Smith</th><th>Brandon Martin</th><th>Erin Dickson</th></tr></thead><tbody><tr><td>Harrison, Richardson and Wilson</td><td>579</td><td>1334</td><td>654</td><td>1194</td><td>1184</td><td>1362</td><td>682</td></tr><tr><td>Reed LLC</td><td>504</td><td>849%</td><td>701%</td><td>965</td><td>421</td><td>286%</td><td>177%</td></tr><tr><td>Klein LLC</td><td>1153</td><td>85</td><td>1041</td><td>554</td><td>900</td><td>1435%</td><td>210%</td></tr><tr><td>Gonzalez Inc</td><td>601%</td><td>1461</td><td>1145</td><td>1586</td><td>1192</td><td>1205</td><td>1101</td></tr><tr><td>Watson, Brown and Long</td><td>1154%</td><td>1303%</td><td>1334%</td><td>1046</td><td>648</td><td>468</td><td>157</td></tr></tbody><tfoot>Creation: 12Jul2008 Fiji</tfoot></table>\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:15:57.339625Z",
     "start_time": "2024-09-27T20:15:55.538614Z"
    }
   },
   "cell_type": "code",
   "source": "pred = translate_greedy_search(transformer, val_idx, html_tokenizer, json_tokenizer)",
   "id": "8b7f5f3b7dc002a4",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:16:01.907921Z",
     "start_time": "2024-09-27T20:16:01.896433Z"
    }
   },
   "cell_type": "code",
   "source": "json_data_str[idx]",
   "id": "2a68e0cb6da7e719",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{][\"body\"][:][{][\"content\"][:][[]\"1560\"[,]\"1012\"[,]\"694\"[,]\"800\"[,]\"240\"[,]\"1371\"[,]\"314%\"[,]\"342%\"[,]\"1204%\"[,]\"189%\"[,]\"1536\"[,]\"1349%\"[]][,][\"headers\"][:][{][\"col\"][:][[]\"Lopez-Foster\"[,]\"Bolton-Thompson\"[]][,][\"row\"][:][[]\"Nancy Vasquez\"[,]\"Jason Martinez\"[,]\"Brittany Mcbride\"[,]\"Victor Phillips\"[,]\"Matthew Luna\"[,]\"Tina Smith\"[]][}][}][,][\"footer\"][:][{][\"table_creation_date:\"][:]\"20Jan2022\"[,][\"text\"][:]\"Creation: 20Jan2022 Madagascar\"[}][,][\"header\"][:][{][\"table_id\"][:]\"29\"[,][\"text\"][:]\"Table 29 Volunteer coordinator\"[}][}]'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T20:16:04.635394Z",
     "start_time": "2024-09-27T20:16:04.621640Z"
    }
   },
   "cell_type": "code",
   "source": "pred[5:-5]",
   "id": "4679ba9464bc240d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[}][\"table_id\"][\"row\"][}][{][\"row\"][\"table_creation_date:\"]FfSLF[\"text\"]FdaSLF[\"text\"]FdfLF[\"text\"]FSdPLF[\"text\"]FdSLF[\"text\"]FdSLF[\"text\"]FdSLF[\"text\"]FdaLF[\"text\"]FfSdF[\"text\"]FaLF[\"text\"]FaakF[\"text\"]FncbqW,WnXF[\"text\"]F1(q/7pW1(QF[\"text\"]F1cc7W1(F[\"text\"]F1cQc)pW1pLF[\"text\"]F1\\'LF[\"text\"]F1pQqpW1c\\'F[\"text\"]Fhp/DqpF[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1pxxpW15pF[\"text\"]F1p\\'F[\"text\"]F1pxpiple\\nc(vep47e,c7F[\"text\"]F1(QcF[\"text\"]F1(QcF[\"text\"]F1pQqcennXF[\"text\"]F1(QcF[\"text\"]F1(pF[\"text\"]F1(pF[\"text\"]F1p\\'F[\"text\"]F1pQqpW1(F[\"text\"]F1pQc/ep47e\\nF[\"text\"]F1pQqp/YF[\"text\"]F1pDtpiF[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1pxxpe\\nenF[\"text\"]F1\\'p4pipe\\nD54F[\"text\"]F1pxpe64DF[\"text\"]F\\npicenx7F[\"text\"]F\\ncQe1qDcF[\"text\"]F1pQqpe1cc7F[\"text\"]F1pQqpe1cc7F[\"text\"]F1pQqpe1cc7F[\"text\"]F1pQqpe1(F[\"text\"]F1p\\'F[\"text\"]F1pQqpe1(F[\"text\"]F1pQqpe1pD5F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1p\\'F[\"text\"]F1ple:pD5Qep47e1pD5F[\"text\"]F1p\\'F[\"text\"]F1plehpiDx5/F[\"t'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-23T22:33:31.930494Z",
     "start_time": "2024-09-23T22:33:31.925393Z"
    }
   },
   "cell_type": "code",
   "source": "pred[5:-5] == json_data_str[idx]",
   "id": "6ff7f7c86c843ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T21:36:36.072125Z",
     "start_time": "2024-09-27T21:31:36.971164Z"
    }
   },
   "cell_type": "code",
   "source": "translate_beam_search(transformer, val_idx, html_tokenizer, json_tokenizer)",
   "id": "51b3c0c9c4a86813",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5560: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[481, 16, 32]' is invalid for input of size 123136",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtranslate_beam_search\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhtml_tokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjson_tokenizer\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\html2json\\seq2seq\\utils.py:121\u001B[0m, in \u001B[0;36mtranslate_beam_search\u001B[1;34m(model, src_sentence, html_tokenizer, json_tokenizer)\u001B[0m\n\u001B[0;32m    119\u001B[0m num_tokens \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    120\u001B[0m src_mask \u001B[38;5;241m=\u001B[39m (torch\u001B[38;5;241m.\u001B[39mzeros(num_tokens, num_tokens))\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mbool)\n\u001B[1;32m--> 121\u001B[0m tgt_tokens \u001B[38;5;241m=\u001B[39m \u001B[43mbeam_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msrc_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mCLS_TOKEN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mSEP_TOKEN\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeam_width\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(json_tokenizer\u001B[38;5;241m.\u001B[39mdecode(tgt_tokens\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()))\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\html2json\\seq2seq\\utils.py:83\u001B[0m, in \u001B[0;36mbeam_search\u001B[1;34m(model, src, src_mask, start_token, end_token, beam_width, max_length)\u001B[0m\n\u001B[0;32m     80\u001B[0m tgt \u001B[38;5;241m=\u001B[39m seq\u001B[38;5;241m.\u001B[39mto(DEVICE)  \u001B[38;5;66;03m# Add batch dimension\u001B[39;00m\n\u001B[0;32m     81\u001B[0m tgt_mask \u001B[38;5;241m=\u001B[39m (Transformer\u001B[38;5;241m.\u001B[39mgenerate_square_subsequent_mask(tgt\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m))\n\u001B[0;32m     82\u001B[0m             \u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mbool))\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[1;32m---> 83\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# Decode the current sequence\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# out = model.decode(tgt, memory)  # Shape (1, seq_len, d_model)\u001B[39;00m\n\u001B[0;32m     87\u001B[0m logits \u001B[38;5;241m=\u001B[39m out[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]  \u001B[38;5;66;03m# Shape (1, vocab_size)\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\html2json\\seq2seq\\models.py:97\u001B[0m, in \u001B[0;36mSeq2SeqTransformer.decode\u001B[1;34m(self, tgt, memory, tgt_mask)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecode\u001B[39m(\u001B[38;5;28mself\u001B[39m, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n\u001B[1;32m---> 97\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpositional_encoding\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     98\u001B[0m \u001B[43m                      \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtgt_tok_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtgt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     99\u001B[0m \u001B[43m                      \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:495\u001B[0m, in \u001B[0;36mTransformerDecoder.forward\u001B[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[0;32m    492\u001B[0m tgt_is_causal \u001B[38;5;241m=\u001B[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001B[0;32m    494\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m mod \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m--> 495\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mmod\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mtgt_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtgt_is_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m                 \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    503\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(output)\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:891\u001B[0m, in \u001B[0;36mTransformerDecoderLayer.forward\u001B[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001B[0m\n\u001B[0;32m    889\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    890\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm1(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001B[1;32m--> 891\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm2(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_mha_block\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_key_padding_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmemory_is_causal\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    892\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm3(x \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ff_block(x))\n\u001B[0;32m    894\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:909\u001B[0m, in \u001B[0;36mTransformerDecoderLayer._mha_block\u001B[1;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001B[0m\n\u001B[0;32m    907\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_mha_block\u001B[39m(\u001B[38;5;28mself\u001B[39m, x: Tensor, mem: Tensor,\n\u001B[0;32m    908\u001B[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 909\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmultihead_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmem\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmem\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    910\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    911\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    912\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    913\u001B[0m \u001B[43m                            \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    914\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout2(x)\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1275\u001B[0m, in \u001B[0;36mMultiheadAttention.forward\u001B[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   1261\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mmulti_head_attention_forward(\n\u001B[0;32m   1262\u001B[0m         query, key, value, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_dim, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_heads,\n\u001B[0;32m   1263\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_weight, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39min_proj_bias,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1272\u001B[0m         average_attn_weights\u001B[38;5;241m=\u001B[39maverage_attn_weights,\n\u001B[0;32m   1273\u001B[0m         is_causal\u001B[38;5;241m=\u001B[39mis_causal)\n\u001B[0;32m   1274\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1275\u001B[0m     attn_output, attn_output_weights \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmulti_head_attention_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1276\u001B[0m \u001B[43m        \u001B[49m\u001B[43mquery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43membed_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1277\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_weight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43min_proj_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1278\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_k\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias_v\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_zero_attn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1279\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mout_proj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtraining\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1281\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkey_padding_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkey_padding_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1282\u001B[0m \u001B[43m        \u001B[49m\u001B[43mneed_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mneed_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage_attn_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage_attn_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1285\u001B[0m \u001B[43m        \u001B[49m\u001B[43mis_causal\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_causal\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1286\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first \u001B[38;5;129;01mand\u001B[39;00m is_batched:\n\u001B[0;32m   1287\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m attn_output\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m0\u001B[39m), attn_output_weights\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:5466\u001B[0m, in \u001B[0;36mmulti_head_attention_forward\u001B[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001B[0m\n\u001B[0;32m   5464\u001B[0m q \u001B[38;5;241m=\u001B[39m q\u001B[38;5;241m.\u001B[39mview(tgt_len, bsz \u001B[38;5;241m*\u001B[39m num_heads, head_dim)\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   5465\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m static_k \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 5466\u001B[0m     k \u001B[38;5;241m=\u001B[39m \u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mview\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbsz\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnum_heads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_dim\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   5467\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   5468\u001B[0m     \u001B[38;5;66;03m# TODO finish disentangling control flow so we don't do in-projections when statics are passed\u001B[39;00m\n\u001B[0;32m   5469\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m static_k\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m) \u001B[38;5;241m==\u001B[39m bsz \u001B[38;5;241m*\u001B[39m num_heads, \\\n\u001B[0;32m   5470\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexpecting static_k.size(0) of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbsz\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39mnum_heads\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstatic_k\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: shape '[481, 16, 32]' is invalid for input of size 123136"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T22:58:58.527756Z",
     "start_time": "2024-09-27T22:58:36.788597Z"
    }
   },
   "cell_type": "code",
   "source": "bleu = load(\"bleu\")",
   "id": "ef9c0aa5745083b5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/a2/e7/cbca9e2d2590eb9b5aa8f7ebabe1beb1498f9462d2ecede5c9fd9735faaf/evaluate-0.4.3-py3-none-any.whl.metadata\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Obtaining dependency information for datasets>=2.0.0 from https://files.pythonhosted.org/packages/be/3e/e58d4db4cfe71e3ed07d169af24db30cfd582e16f977378bd43fd7ec1998/datasets-3.0.1-py3-none-any.whl.metadata\n",
      "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from evaluate) (1.26.3)\n",
      "Collecting dill (from evaluate)\n",
      "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl.metadata\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from evaluate) (4.66.5)\n",
      "Collecting xxhash (from evaluate)\n",
      "  Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/d9/6b/1c443fe6cfeb4ad1dcf231cdec96eb94fb43d6498b4469ed8b51f8b59a37/xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Obtaining dependency information for multiprocess from https://files.pythonhosted.org/packages/0a/7d/a988f258104dcd2ccf1ed40fdc97e26c4ac351eeaf81d76e266c52d84e2f/multiprocess-0.70.16-py312-none-any.whl.metadata\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from evaluate) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from evaluate) (0.25.0)\n",
      "Requirement already satisfied: packaging in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for pyarrow>=15.0.0 from https://files.pythonhosted.org/packages/ae/49/baafe2a964f663413be3bd1cf5c45ed98c5e42e804e2328e18f4570027c1/pyarrow-17.0.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting aiohttp (from datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/51/69/bdefa27db2ceb6c77a296fd075485ab191980ab6c3593132c2606b6678a7/aiohttp-3.10.7-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading aiohttp-3.10.7-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from pandas->evaluate) (2024.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for aiohappyeyeballs>=2.3.0 from https://files.pythonhosted.org/packages/13/64/40165ff77ade5203284e3015cf88e11acb07d451f6bf83fff71192912a0d/aiohappyeyeballs-2.4.2-py3-none-any.whl.metadata\n",
      "  Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for aiosignal>=1.1.2 from https://files.pythonhosted.org/packages/76/ac/a7305707cb852b7e16ff80eaf5692309bde30e2b1100a1fcacdc8f731d97/aiosignal-1.3.1-py3-none-any.whl.metadata\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/3e/dc/96647994a013bc72f3d453abab18340b7f5e222b7b7291e3697ca1fcfbd5/frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for multidict<7.0,>=4.5 from https://files.pythonhosted.org/packages/a3/bf/f332a13486b1ed0496d624bcc7e8357bb8053823e8cd4b9a18edc1d97e73/multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Obtaining dependency information for yarl<2.0,>=1.12.0 from https://files.pythonhosted.org/packages/9c/c0/7329799080d7e0bf7b10db417900701ba6810e78a249aef1f4bf3fc2cccb/yarl-1.13.1-cp312-cp312-win_amd64.whl.metadata\n",
      "  Downloading yarl-1.13.1-cp312-cp312-win_amd64.whl.metadata (52 kB)\n",
      "     ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/52.5 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/52.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 52.5/52.5 kB 681.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\workspace\\characterbasednmt\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "   ---------------------------------------- 0.0/84.0 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/84.0 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 30.7/84.0 kB 660.6 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 41.0/84.0 kB 393.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 71.7/84.0 kB 393.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 84.0/84.0 kB 428.9 kB/s eta 0:00:00\n",
      "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
      "   ---------------------------------------- 0.0/471.6 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/471.6 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/471.6 kB 640.0 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 61.4/471.6 kB 544.7 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 71.7/471.6 kB 435.7 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 112.6/471.6 kB 502.0 kB/s eta 0:00:01\n",
      "   --------- ---------------------------- 122.9/471.6 kB 450.6 kB/s eta 0:00:01\n",
      "   ------------ ------------------------- 153.6/471.6 kB 482.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 174.1/471.6 kB 498.0 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 204.8/471.6 kB 497.6 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 204.8/471.6 kB 497.6 kB/s eta 0:00:01\n",
      "   ------------------ ------------------- 225.3/471.6 kB 458.5 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 256.0/471.6 kB 462.0 kB/s eta 0:00:01\n",
      "   ---------------------- --------------- 276.5/471.6 kB 472.9 kB/s eta 0:00:01\n",
      "   ------------------------ ------------- 307.2/471.6 kB 486.8 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 337.9/471.6 kB 498.8 kB/s eta 0:00:01\n",
      "   ---------------------------- --------- 358.4/471.6 kB 506.0 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 389.1/471.6 kB 515.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 440.3/471.6 kB 529.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 471.6/471.6 kB 536.4 kB/s eta 0:00:00\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "   ---------------------------------------- 0.0/146.7 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 30.7/146.7 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------- ---------------------- 61.4/146.7 kB 825.8 kB/s eta 0:00:01\n",
      "   --------------------- ----------------- 81.9/146.7 kB 770.8 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 112.6/146.7 kB 731.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 146.7/146.7 kB 730.0 kB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Downloading aiohttp-3.10.7-cp312-cp312-win_amd64.whl (378 kB)\n",
      "   ---------------------------------------- 0.0/378.9 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 30.7/378.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ------ -------------------------------- 61.4/378.9 kB 812.7 kB/s eta 0:00:01\n",
      "   --------- ----------------------------- 92.2/378.9 kB 744.7 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 143.4/378.9 kB 774.0 kB/s eta 0:00:01\n",
      "   --------------- ---------------------- 153.6/378.9 kB 762.6 kB/s eta 0:00:01\n",
      "   -------------------- ----------------- 204.8/378.9 kB 778.2 kB/s eta 0:00:01\n",
      "   ----------------------- -------------- 235.5/378.9 kB 758.5 kB/s eta 0:00:01\n",
      "   --------------------------- ---------- 276.5/378.9 kB 811.7 kB/s eta 0:00:01\n",
      "   ------------------------------ ------- 307.2/378.9 kB 824.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 358.4/378.9 kB 825.0 kB/s eta 0:00:01\n",
      "   -------------------------------------- 378.9/378.9 kB 813.8 kB/s eta 0:00:00\n",
      "Downloading pyarrow-17.0.0-cp312-cp312-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.1 MB 1.3 MB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/25.1 MB 1.1 MB/s eta 0:00:23\n",
      "   ---------------------------------------- 0.1/25.1 MB 871.5 kB/s eta 0:00:29\n",
      "   ---------------------------------------- 0.1/25.1 MB 901.1 kB/s eta 0:00:28\n",
      "   ---------------------------------------- 0.2/25.1 MB 876.1 kB/s eta 0:00:29\n",
      "   ---------------------------------------- 0.2/25.1 MB 831.5 kB/s eta 0:00:30\n",
      "   ---------------------------------------- 0.2/25.1 MB 885.4 kB/s eta 0:00:29\n",
      "   ---------------------------------------- 0.3/25.1 MB 886.2 kB/s eta 0:00:29\n",
      "    --------------------------------------- 0.3/25.1 MB 912.8 kB/s eta 0:00:28\n",
      "    --------------------------------------- 0.4/25.1 MB 916.6 kB/s eta 0:00:27\n",
      "    --------------------------------------- 0.4/25.1 MB 937.3 kB/s eta 0:00:27\n",
      "    --------------------------------------- 0.5/25.1 MB 952.1 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.5/25.1 MB 964.2 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.6/25.1 MB 965.7 kB/s eta 0:00:26\n",
      "    --------------------------------------- 0.6/25.1 MB 975.2 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.7/25.1 MB 983.4 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.7/25.1 MB 975.9 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 0.7/25.1 MB 983.0 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.8/25.1 MB 983.4 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.8/25.1 MB 989.4 kB/s eta 0:00:25\n",
      "   - -------------------------------------- 0.9/25.1 MB 1.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 0.9/25.1 MB 1.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 1.0/25.1 MB 1.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 1.0/25.1 MB 1.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 1.1/25.1 MB 1.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 1.1/25.1 MB 1.0 MB/s eta 0:00:24\n",
      "   - -------------------------------------- 1.2/25.1 MB 1.0 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 1.2/25.1 MB 1.0 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 1.3/25.1 MB 1.1 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 1.4/25.1 MB 1.1 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 1.4/25.1 MB 1.1 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 1.4/25.1 MB 1.1 MB/s eta 0:00:23\n",
      "   -- ------------------------------------- 1.5/25.1 MB 1.1 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.5/25.1 MB 1.1 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.6/25.1 MB 1.1 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.6/25.1 MB 1.1 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.7/25.1 MB 1.1 MB/s eta 0:00:22\n",
      "   -- ------------------------------------- 1.8/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 1.8/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 1.8/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.9/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 1.9/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 2.0/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 2.0/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 2.1/25.1 MB 1.1 MB/s eta 0:00:21\n",
      "   --- ------------------------------------ 2.2/25.1 MB 1.2 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 2.2/25.1 MB 1.2 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 2.3/25.1 MB 1.2 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 2.4/25.1 MB 1.2 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 2.4/25.1 MB 1.2 MB/s eta 0:00:20\n",
      "   --- ------------------------------------ 2.5/25.1 MB 1.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 2.5/25.1 MB 1.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 2.6/25.1 MB 1.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 2.7/25.1 MB 1.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 2.7/25.1 MB 1.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 2.8/25.1 MB 1.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 1.2 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 2.9/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 3.0/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 3.1/25.1 MB 1.2 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.2/25.1 MB 1.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.3/25.1 MB 1.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.3/25.1 MB 1.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.4/25.1 MB 1.3 MB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 3.5/25.1 MB 1.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 3.5/25.1 MB 1.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 3.6/25.1 MB 1.3 MB/s eta 0:00:17\n",
      "   ----- ---------------------------------- 3.7/25.1 MB 1.3 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 3.8/25.1 MB 1.3 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 3.9/25.1 MB 1.3 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 3.9/25.1 MB 1.3 MB/s eta 0:00:17\n",
      "   ------ --------------------------------- 4.0/25.1 MB 1.3 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 4.1/25.1 MB 1.3 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 4.1/25.1 MB 1.3 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 4.2/25.1 MB 1.4 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 4.3/25.1 MB 1.3 MB/s eta 0:00:16\n",
      "   ------ --------------------------------- 4.4/25.1 MB 1.4 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 4.4/25.1 MB 1.4 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 4.5/25.1 MB 1.4 MB/s eta 0:00:16\n",
      "   ------- -------------------------------- 4.6/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 4.7/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 4.8/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 4.8/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 4.9/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 5.0/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 5.1/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 5.2/25.1 MB 1.4 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 5.3/25.1 MB 1.4 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 5.4/25.1 MB 1.4 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 5.4/25.1 MB 1.4 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 5.5/25.1 MB 1.4 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 5.6/25.1 MB 1.4 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 5.7/25.1 MB 1.5 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 5.7/25.1 MB 1.5 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 5.8/25.1 MB 1.5 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 5.9/25.1 MB 1.5 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 6.0/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 6.1/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   --------- ------------------------------ 6.2/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.3/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.4/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.4/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.6/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.6/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.7/25.1 MB 1.5 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 6.8/25.1 MB 1.5 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 6.9/25.1 MB 1.5 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.0/25.1 MB 1.5 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.1/25.1 MB 1.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.2/25.1 MB 1.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.3/25.1 MB 1.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.4/25.1 MB 1.6 MB/s eta 0:00:12\n",
      "   ----------- ---------------------------- 7.5/25.1 MB 1.6 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 7.6/25.1 MB 1.6 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 7.7/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 7.8/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 7.9/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 8.0/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 8.1/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 8.2/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 8.3/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 8.4/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 8.5/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 8.6/25.1 MB 1.6 MB/s eta 0:00:11\n",
      "   ------------- -------------------------- 8.7/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 8.8/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 8.9/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 9.0/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 9.1/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 9.2/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 9.3/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 9.4/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 9.5/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 9.7/25.1 MB 1.7 MB/s eta 0:00:10\n",
      "   --------------- ------------------------ 9.7/25.1 MB 1.7 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 9.8/25.1 MB 1.7 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 10.0/25.1 MB 1.7 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 10.0/25.1 MB 1.7 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.2/25.1 MB 1.7 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.3/25.1 MB 1.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.4/25.1 MB 1.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.5/25.1 MB 1.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 10.6/25.1 MB 1.8 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.7/25.1 MB 1.8 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 10.8/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 11.0/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 11.1/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 11.2/25.1 MB 1.9 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 11.3/25.1 MB 2.0 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 11.4/25.1 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.6/25.1 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.7/25.1 MB 2.0 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 11.9/25.1 MB 2.1 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.0/25.1 MB 2.1 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.2/25.1 MB 2.1 MB/s eta 0:00:07\n",
      "   ------------------- -------------------- 12.3/25.1 MB 2.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 12.4/25.1 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.6/25.1 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.8/25.1 MB 2.2 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 12.9/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 13.1/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.2/25.1 MB 2.3 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 13.4/25.1 MB 2.3 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.5/25.1 MB 2.4 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 13.7/25.1 MB 2.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 13.8/25.1 MB 2.4 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.0/25.1 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.2/25.1 MB 2.5 MB/s eta 0:00:05\n",
      "   ---------------------- ----------------- 14.3/25.1 MB 2.5 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 14.6/25.1 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 14.8/25.1 MB 2.6 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 14.9/25.1 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.1/25.1 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.3/25.1 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 15.5/25.1 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 15.8/25.1 MB 2.8 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.0/25.1 MB 2.9 MB/s eta 0:00:04\n",
      "   ------------------------- -------------- 16.2/25.1 MB 2.9 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 16.4/25.1 MB 3.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.6/25.1 MB 3.0 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 16.8/25.1 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.0/25.1 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.3/25.1 MB 3.2 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 17.5/25.1 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.7/25.1 MB 3.3 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 17.9/25.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 18.1/25.1 MB 3.4 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 18.4/25.1 MB 3.5 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 18.7/25.1 MB 3.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.1/25.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 19.3/25.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.6/25.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 19.9/25.1 MB 3.9 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.2/25.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 20.5/25.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 20.7/25.1 MB 4.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 21.1/25.1 MB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.4/25.1 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.6/25.1 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 21.9/25.1 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 22.2/25.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 22.6/25.1 MB 4.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.0/25.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.2/25.1 MB 5.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.5/25.1 MB 5.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 23.8/25.1 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/25.1 MB 5.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.3/25.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/25.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/25.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 5.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading aiohappyeyeballs-2.4.2-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Downloading yarl-1.13.1-cp312-cp312-win_amd64.whl (111 kB)\n",
      "   ---------------------------------------- 0.0/111.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 111.7/111.7 kB 6.3 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, pyarrow, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets, evaluate\n",
      "Successfully installed aiohappyeyeballs-2.4.2 aiohttp-3.10.7 aiosignal-1.3.1 datasets-3.0.1 dill-0.3.8 evaluate-0.4.3 frozenlist-1.4.1 multidict-6.1.0 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0 yarl-1.13.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predictions = [translate_greedy_search(transformer, html_data_str[idx], html_tokenizer, json_tokenizer) for i, idx in enumerate(val_idx) if i <= 10]\n",
    "references = [json_data_str[idx] for i, idx in enumerate(val_idx) if i <= 10]"
   ],
   "id": "ef359151411c3c35"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-27T22:59:14.830018Z",
     "start_time": "2024-09-27T22:59:08.989942Z"
    }
   },
   "cell_type": "code",
   "source": "bleu.compute(predictions=predictions, references=references)",
   "id": "a4a009a075a2a69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "363361f63b024bc598cdb9152a68e2cf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7b5f8ee70a44c4ab474e46c122cd44d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fdd67a13f1e415f9991f63d16dab3b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
