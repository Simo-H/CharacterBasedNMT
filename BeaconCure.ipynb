{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:40:07.618603Z",
     "start_time": "2024-09-22T07:40:07.614498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "# !pip install transformers"
   ],
   "id": "d3792bddd0b9d582",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:40:10.128233Z",
     "start_time": "2024-09-22T07:40:07.676018Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "id": "8e9f1a92b564c2cc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:40:10.135937Z",
     "start_time": "2024-09-22T07:40:10.130242Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.empty_cache()",
   "id": "a7863f0e92dc6675",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:40:10.142753Z",
     "start_time": "2024-09-22T07:40:10.137947Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# In this notebook we train a translation model from data in html format to json format.\n",
    "# the model architecture is a transformer model with an encoder-decoder architecture.\n",
    "# the model is charcter based and is trained on a dataset of html files and their corresponding json files.\n",
    "# The steps are as follows:\n",
    "# 1. remove all attributes from html tags as they are not needed for translation\n",
    "# 2. create a tokenizer for the html data where each tag is a token along with the characters inside the tags.\n",
    "# 3. create a tokenizer for the json data where each key is a token along with the characters inside the values.\n",
    "# 4. create a transformer model with an encoder-decoder architecture.\n",
    "# 5. train the model on the dataset using teacher forcing and a custom loss function - the loss function is a combination of cross entropy loss and a custom loss function that penalizes the model for creating the right structure.\n",
    "# 6. evaluate the model on the test set.\n",
    "# 7. save the model for future use."
   ],
   "id": "ebd0818dbfc27789",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:40:11.069245Z",
     "start_time": "2024-09-22T07:40:10.147336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a torch dataset from the html and json files\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "from charactertokenizer import CharacterTokenizer"
   ],
   "id": "3d97ae79358792c",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:40:11.073790Z",
     "start_time": "2024-09-22T07:40:11.070249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def remove_attrs(soup):\n",
    "    for tag in soup.find_all(True):\n",
    "        tag.attrs = {}\n",
    "    return soup"
   ],
   "id": "fcf5b724dd04808a",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:40:11.078959Z",
     "start_time": "2024-09-22T07:40:11.074796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# removing trailing and leading whitespaces from tag.strings for all html data\n",
    "def remove_whitespace(soup):\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.string is None:\n",
    "            continue\n",
    "        tag.string = tag.string.strip()\n",
    "    return soup"
   ],
   "id": "66d8b722ad8227fb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:27.821910Z",
     "start_time": "2024-09-22T07:40:11.079967Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load all html a store it in memory to save time in io operations\n",
    "html_data = []\n",
    "html_str_data = []\n",
    "for html_file in os.listdir('./beaconcure_data/tables'):\n",
    "    with open(f'./beaconcure_data/tables/{html_file}') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        soup = remove_attrs(soup)\n",
    "        soup = remove_whitespace(soup)\n",
    "        html_data.append(soup)\n",
    "        #### TODO: remove the newlines between tags in the html files but not from the string data, e.g. from the soup object.\n",
    "        html_str_data.append(str(soup).replace(\">\\n<\", \"><\"))"
   ],
   "id": "4f6cfdad305d1b28",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:31.381340Z",
     "start_time": "2024-09-22T07:41:27.823424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# building a tokenizer for the html data, each tag is a token and the characters inside the tags are also tokens\n",
    "# get a set of all tags in the html files\n",
    "html_regular_tokens = set()\n",
    "html_special_tokens = set()\n",
    "for html_file in html_data:\n",
    "    # add all tags to the set\n",
    "    for tag in html_file.find_all(True):\n",
    "        html_special_tokens.add(\"<{tag_name}>\".format(tag_name = tag.name))\n",
    "        html_special_tokens.add(\"</{tag_name}>\".format(tag_name = tag.name))\n",
    "    # add all characters to the set\n",
    "    for char in html_file.get_text():\n",
    "        html_regular_tokens.add(char)"
   ],
   "id": "ed1e16b5d91492e5",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:31.387339Z",
     "start_time": "2024-09-22T07:41:31.382345Z"
    }
   },
   "cell_type": "code",
   "source": "html_regular_tokens",
   "id": "d4b7c163591ca4f3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:31.396533Z",
     "start_time": "2024-09-22T07:41:31.390346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a tokenizer for the html data\n",
    "html_tokenizer = CharacterTokenizer(html_regular_tokens, html_special_tokens, 10000, padding=True)"
   ],
   "id": "cc22f5974d120798",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:31.402144Z",
     "start_time": "2024-09-22T07:41:31.397539Z"
    }
   },
   "cell_type": "code",
   "source": "html_str_data[0]",
   "id": "e9add0882ad076ff",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><caption>Table 59.99.9.62 Loss adjuster, chartered</caption><thead><tr><th></th><th>Daniel Brown</th><th>Shane Barnes DDS</th><th>Nicole Carpenter</th><th>Kristin Duarte</th></tr><th></th><th>programmer</th><th>Carpenter</th><th>singer</th><th>actor</th>\\n\\n</thead><tbody><tr><td>Roberts LLC</td><td>1060</td><td>37</td><td>1593</td><td>1364</td></tr></tbody><tfoot>modified: 5Feb2013</tfoot><tfoot>Creation: 3Feb2013 Chad</tfoot></table>\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:31.410018Z",
     "start_time": "2024-09-22T07:41:31.403658Z"
    }
   },
   "cell_type": "code",
   "source": "html_tokenizer.decode(html_tokenizer(html_str_data[0])['input_ids'])",
   "id": "ed319e312b8f6769",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]<table><caption>Table 59.99.9.62 Loss adjuster, chartered</caption><thead><tr><th></th><th>Daniel Brown</th><th>Shane Barnes DDS</th><th>Nicole Carpenter</th><th>Kristin Duarte</th></tr><th></th><th>programmer</th><th>Carpenter</th><th>singer</th><th>actor</th>\\n\\n</thead><tbody><tr><td>Roberts LLC</td><td>1060</td><td>37</td><td>1593</td><td>1364</td></tr></tbody><tfoot>modified: 5Feb2013</tfoot><tfoot>Creation: 3Feb2013 Chad</tfoot></table>\\n[SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:31.415906Z",
     "start_time": "2024-09-22T07:41:31.411028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def encode(self, obj):\n",
    "        def custom_format(value):\n",
    "            if isinstance(value, dict):\n",
    "                items = [f'[{json.dumps(k)}][:]{custom_format(v)}' for k, v in value.items()]\n",
    "                return f'[{{]{\"[,]\".join(items)}[}}]'\n",
    "            elif isinstance(value, list):\n",
    "                items = [custom_format(v) for v in value]\n",
    "                return f'[[]{\"[,]\".join(items)}[]]'\n",
    "            else:\n",
    "                return json.dumps(value)\n",
    "        \n",
    "        return custom_format(obj)"
   ],
   "id": "351c1270c56fac58",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:37.304376Z",
     "start_time": "2024-09-22T07:41:31.416913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "json_data = []\n",
    "json_str_data = []\n",
    "for json_file in os.listdir('./beaconcure_data/metadata'):\n",
    "    with open(f'./beaconcure_data/metadata/{json_file}') as f:\n",
    "        parsed_json = json.load(f)\n",
    "        json_data.append(parsed_json)\n",
    "        json_str_data.append(json.dumps(parsed_json, cls=CustomJSONEncoder))"
   ],
   "id": "e1b90de897cc46f3",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:37.309060Z",
     "start_time": "2024-09-22T07:41:37.305389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_keys(dictionary):\n",
    "    keys = set()\n",
    "    if isinstance(dictionary, list):\n",
    "        for item in dictionary:\n",
    "            keys.update(get_keys(item))\n",
    "    elif isinstance(dictionary, dict):\n",
    "        for key in dictionary:\n",
    "            keys.add(key)\n",
    "            keys.update(get_keys(dictionary[key]))\n",
    "    return keys"
   ],
   "id": "981c149f27b508d9",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:37.314269Z",
     "start_time": "2024-09-22T07:41:37.310065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_values(dictionary):\n",
    "    values = set()\n",
    "    if isinstance(dictionary, list):\n",
    "        for item in dictionary:\n",
    "            values.update(get_values(item))\n",
    "    elif isinstance(dictionary, dict):\n",
    "        for key, value in dictionary.items():\n",
    "            # values.add(value)\n",
    "            values.update(get_values(dictionary[key]))\n",
    "    else:\n",
    "        for value in dictionary:\n",
    "            values.add(value)\n",
    "    return values"
   ],
   "id": "733665dd3da9a939",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.916970Z",
     "start_time": "2024-09-22T07:41:37.315276Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# building a tokenizer for the html data, each tag is a token and the characters inside the tags are also tokens\n",
    "# get a set of all tags in the html files\n",
    "json_regular_tokens = set()\n",
    "json_special_tokens = set()\n",
    "json_special_tokens.add(\"[{]\")\n",
    "json_special_tokens.add(\"[}]\")\n",
    "json_special_tokens.add(\"[:]\")\n",
    "json_special_tokens.add(\"[,]\")\n",
    "json_special_tokens.add(\"[[]\")\n",
    "json_special_tokens.add(\"[]]\")\n",
    "json_regular_tokens.add(\"\\\"\")\n",
    "json_regular_tokens.add(\"\\\\\")\n",
    "for json_file in json_data:\n",
    "    # add all tags to the set\n",
    "    for key in get_keys(json_file):\n",
    "        json_special_tokens.add(f\"[\\\"{key}\\\"]\")\n",
    "    # add all characters to the set\n",
    "    json_regular_tokens.update(get_values(json_file))"
   ],
   "id": "4bb4b811ea5355e1",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.925613Z",
     "start_time": "2024-09-22T07:41:38.917975Z"
    }
   },
   "cell_type": "code",
   "source": "json_regular_tokens",
   "id": "67ad54b2b9906f1e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n',\n",
       " ' ',\n",
       " '\"',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'K',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'W',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '\\\\',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.930341Z",
     "start_time": "2024-09-22T07:41:38.926622Z"
    }
   },
   "cell_type": "code",
   "source": "json_special_tokens",
   "id": "1a376b39435545e9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[\"body\"]',\n",
       " '[\"col\"]',\n",
       " '[\"content\"]',\n",
       " '[\"footer\"]',\n",
       " '[\"header\"]',\n",
       " '[\"headers\"]',\n",
       " '[\"row\"]',\n",
       " '[\"table_creation_date:\"]',\n",
       " '[\"table_id\"]',\n",
       " '[\"text\"]',\n",
       " '[,]',\n",
       " '[:]',\n",
       " '[[]',\n",
       " '[]]',\n",
       " '[{]',\n",
       " '[}]'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.936139Z",
     "start_time": "2024-09-22T07:41:38.931350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a tokenizer for the json data\n",
    "json_tokenizer = CharacterTokenizer(json_regular_tokens, json_special_tokens, 10000)"
   ],
   "id": "eb3ed893101cec92",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.940663Z",
     "start_time": "2024-09-22T07:41:38.937144Z"
    }
   },
   "cell_type": "code",
   "source": "json_str_data[0]",
   "id": "f1652b2ad41fd432",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{][\"body\"][:][{][\"content\"][:][[]\"1060\"[,]\"37\"[,]\"1593\"[,]\"1364\"[]][,][\"headers\"][:][{][\"col\"][:][[]\"Roberts LLC\"[]][,][\"row\"][:][[]\"Daniel Brown\"[,]\"Shane Barnes DDS\"[,]\"Nicole Carpenter\"[,]\"Kristin Duarte\"[,]\"programmer\"[,]\"Carpenter\"[,]\"singer\"[,]\"actor\"[]][}][}][,][\"footer\"][:][{][\"table_creation_date:\"][:]\"3Feb2013\"[,][\"text\"][:]\"modified: 5Feb2013\\\\nCreation: 3Feb2013 Chad\"[}][,][\"header\"][:][{][\"table_id\"][:]\"59.99.9.62\"[,][\"text\"][:]\"Table 59.99.9.62 Loss adjuster, chartered\"[}][}]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.946748Z",
     "start_time": "2024-09-22T07:41:38.941668Z"
    }
   },
   "cell_type": "code",
   "source": "json_tokenizer.decode(json_tokenizer(json_str_data[0])['input_ids'])[5:-5]",
   "id": "ea40ba5f836da6b6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{][\"body\"][:][{][\"content\"][:][[]\"1060\"[,]\"37\"[,]\"1593\"[,]\"1364\"[]][,][\"headers\"][:][{][\"col\"][:][[]\"Roberts LLC\"[]][,][\"row\"][:][[]\"Daniel Brown\"[,]\"Shane Barnes DDS\"[,]\"Nicole Carpenter\"[,]\"Kristin Duarte\"[,]\"programmer\"[,]\"Carpenter\"[,]\"singer\"[,]\"actor\"[]][}][}][,][\"footer\"][:][{][\"table_creation_date:\"][:]\"3Feb2013\"[,][\"text\"][:]\"modified: 5Feb2013\\\\nCreation: 3Feb2013 Chad\"[}][,][\"header\"][:][{][\"table_id\"][:]\"59.99.9.62\"[,][\"text\"][:]\"Table 59.99.9.62 Loss adjuster, chartered\"[}][}]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.953264Z",
     "start_time": "2024-09-22T07:41:38.947755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BeaconCureDataset(Dataset):\n",
    "    def __init__(self, html_data, json_data, html_tokenizer, json_tokenizer):\n",
    "        self.html_data = [html_tokenizer.encode(html_str) for html_str in html_data]\n",
    "        self.json_data = [json_tokenizer.encode(json_str) for json_str in json_data]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.html_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.LongTensor(self.html_data[idx]), torch.LongTensor(self.json_data[idx])"
   ],
   "id": "a37608dd4aea776a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:41:38.958099Z",
     "start_time": "2024-09-22T07:41:38.953778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def collate_fn(batch, PAD_TOKEN_HTML, PAD_TOKEN_JSON):\n",
    "    src_batch, tgt_batch = list(zip(*batch))\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_TOKEN_HTML)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_TOKEN_JSON)\n",
    "    return src_batch, tgt_batch"
   ],
   "id": "d16d2a31445bfdfe",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:42:09.841621Z",
     "start_time": "2024-09-22T07:41:38.959120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from functools import partial\n",
    "PAD_TOKEN_HTML = html_tokenizer.get_vocab()['[PAD]']\n",
    "PAD_TOKEN_JSON = json_tokenizer.get_vocab()['[PAD]']\n",
    "\n",
    "collate_fn_partial = partial(collate_fn, PAD_TOKEN_HTML = PAD_TOKEN_HTML, PAD_TOKEN_JSON = PAD_TOKEN_JSON)\n",
    "bc_dataset = BeaconCureDataset(html_str_data, json_str_data, html_tokenizer, json_tokenizer)\n",
    "# train_dataloader = DataLoader(bc_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n"
   ],
   "id": "9a859a086be3779c",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:42:09.847975Z",
     "start_time": "2024-09-22T07:42:09.842322Z"
    }
   },
   "cell_type": "code",
   "source": "len(bc_dataset[0][0])",
   "id": "428f20485c71911b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:42:09.892961Z",
     "start_time": "2024-09-22T07:42:09.850070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 512,\n",
    "                 dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ],
   "id": "7b5e9f76d9c83064",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:02:27.413688Z",
     "start_time": "2024-09-22T19:02:27.403989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    # tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_TOKEN_HTML).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_TOKEN_JSON).transpose(0, 1)\n",
    "    return src_mask, src_padding_mask, tgt_padding_mask"
   ],
   "id": "cea3c8af235acc16",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T19:02:50.229620Z",
     "start_time": "2024-09-22T19:02:50.207586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "len(json_tokenizer)"
   ],
   "id": "606fb9e72cd61e94",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:57:48.815063Z",
     "start_time": "2024-09-22T07:57:48.782068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "SRC_VOCAB_SIZE = len(html_tokenizer)\n",
    "TGT_VOCAB_SIZE = len(json_tokenizer)\n",
    "EMB_SIZE = 32\n",
    "NHEAD = 4\n",
    "FFN_HID_DIM = 64\n",
    "BATCH_SIZE = 128\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_dataloader = DataLoader(bc_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn_partial)\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_JSON)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "id": "2d5bab3550b80ab4",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:57:49.614992Z",
     "start_time": "2024-09-22T07:57:49.609824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for i, (src, tgt) in enumerate(train_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        tgt_mask = model.transformer.generate_square_subsequent_mask(tgt_input.size(0)).to(DEVICE)\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        print(\"Batch: {0}, Loss: {1}\".format(i, loss.item()))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(list(train_dataloader))"
   ],
   "id": "ff44bc7f03ff39d9",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T07:57:50.810524Z",
     "start_time": "2024-09-22T07:57:50.806931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def evaluate(model):\n",
    "#     model.eval()\n",
    "#     losses = 0\n",
    "#     val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "# \n",
    "#     for src, tgt in val_dataloader:\n",
    "#         src = src.to(DEVICE)\n",
    "#         tgt = tgt.to(DEVICE)\n",
    "# \n",
    "#         tgt_input = tgt[:-1, :]\n",
    "# \n",
    "#         src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "# \n",
    "#         logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "# \n",
    "#         tgt_out = tgt[1:, :]\n",
    "#         loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "#         losses += loss.item()\n",
    "# \n",
    "#     return losses / len(list(val_dataloader))"
   ],
   "id": "9e2bf41cb84b5bbd",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-22T16:09:49.111474Z",
     "start_time": "2024-09-22T07:57:51.074030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 18\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    # val_loss = evaluate(transformer)\n",
    "    val_loss = 0\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))"
   ],
   "id": "42cf214838e3d123",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0, Loss: 4.841628074645996\n",
      "Batch: 1, Loss: 4.818394660949707\n",
      "Batch: 2, Loss: 4.791187763214111\n",
      "Batch: 3, Loss: 4.758297443389893\n",
      "Batch: 4, Loss: 4.752079010009766\n",
      "Batch: 5, Loss: 4.723038196563721\n",
      "Batch: 6, Loss: 4.694632530212402\n",
      "Batch: 7, Loss: 4.673187732696533\n",
      "Batch: 8, Loss: 4.654795169830322\n",
      "Batch: 9, Loss: 4.6329264640808105\n",
      "Batch: 10, Loss: 4.614378452301025\n",
      "Batch: 11, Loss: 4.598561763763428\n",
      "Batch: 12, Loss: 4.582469940185547\n",
      "Batch: 13, Loss: 4.582179069519043\n",
      "Batch: 14, Loss: 4.545294284820557\n",
      "Batch: 15, Loss: 4.5380539894104\n",
      "Batch: 16, Loss: 4.521754741668701\n",
      "Batch: 17, Loss: 4.5250749588012695\n",
      "Batch: 18, Loss: 4.501384735107422\n",
      "Batch: 19, Loss: 4.498808860778809\n",
      "Batch: 20, Loss: 4.479899883270264\n",
      "Batch: 21, Loss: 4.461790561676025\n",
      "Batch: 22, Loss: 4.460177898406982\n",
      "Batch: 23, Loss: 4.45826530456543\n",
      "Batch: 24, Loss: 4.438604831695557\n",
      "Batch: 25, Loss: 4.42635440826416\n",
      "Batch: 26, Loss: 4.41569185256958\n",
      "Batch: 27, Loss: 4.420628547668457\n",
      "Batch: 28, Loss: 4.411268711090088\n",
      "Batch: 29, Loss: 4.407686710357666\n",
      "Batch: 30, Loss: 4.406810760498047\n",
      "Batch: 31, Loss: 4.403833389282227\n",
      "Batch: 32, Loss: 4.389873504638672\n",
      "Batch: 33, Loss: 4.388339042663574\n",
      "Batch: 34, Loss: 4.359342575073242\n",
      "Batch: 35, Loss: 4.38861083984375\n",
      "Batch: 36, Loss: 4.361062049865723\n",
      "Batch: 37, Loss: 4.340464115142822\n",
      "Batch: 38, Loss: 4.338154315948486\n",
      "Batch: 39, Loss: 4.340293884277344\n",
      "Batch: 40, Loss: 4.348389625549316\n",
      "Batch: 41, Loss: 4.327496528625488\n",
      "Batch: 42, Loss: 4.313183307647705\n",
      "Batch: 43, Loss: 4.325825214385986\n",
      "Batch: 44, Loss: 4.321479797363281\n",
      "Batch: 45, Loss: 4.32193660736084\n",
      "Batch: 46, Loss: 4.3101983070373535\n",
      "Batch: 47, Loss: 4.312265872955322\n",
      "Batch: 48, Loss: 4.289164066314697\n",
      "Batch: 49, Loss: 4.283970355987549\n",
      "Batch: 50, Loss: 4.282668590545654\n",
      "Batch: 51, Loss: 4.289306640625\n",
      "Batch: 52, Loss: 4.271539688110352\n",
      "Batch: 53, Loss: 4.2755022048950195\n",
      "Batch: 54, Loss: 4.25648832321167\n",
      "Batch: 55, Loss: 4.267944812774658\n",
      "Batch: 56, Loss: 4.263344764709473\n",
      "Batch: 57, Loss: 4.24370813369751\n",
      "Batch: 58, Loss: 4.2585015296936035\n",
      "Batch: 59, Loss: 4.2510600090026855\n",
      "Batch: 60, Loss: 4.245284080505371\n",
      "Batch: 61, Loss: 4.2449188232421875\n",
      "Batch: 62, Loss: 4.242781162261963\n",
      "Batch: 63, Loss: 4.2283196449279785\n",
      "Batch: 64, Loss: 4.228211402893066\n",
      "Batch: 65, Loss: 4.221918106079102\n",
      "Batch: 66, Loss: 4.219032287597656\n",
      "Batch: 67, Loss: 4.216274738311768\n",
      "Batch: 68, Loss: 4.1955132484436035\n",
      "Batch: 69, Loss: 4.213782787322998\n",
      "Batch: 70, Loss: 4.205058574676514\n",
      "Batch: 71, Loss: 4.204983711242676\n",
      "Batch: 72, Loss: 4.196368217468262\n",
      "Batch: 73, Loss: 4.192712783813477\n",
      "Batch: 74, Loss: 4.192377090454102\n",
      "Batch: 75, Loss: 4.164276123046875\n",
      "Batch: 76, Loss: 4.16728401184082\n",
      "Batch: 77, Loss: 4.18628454208374\n",
      "Batch: 78, Loss: 4.179834365844727\n",
      "Batch: 79, Loss: 4.167084217071533\n",
      "Batch: 80, Loss: 4.14816427230835\n",
      "Batch: 81, Loss: 4.162831783294678\n",
      "Batch: 82, Loss: 4.15933084487915\n",
      "Batch: 83, Loss: 4.164059638977051\n",
      "Batch: 84, Loss: 4.164010047912598\n",
      "Batch: 85, Loss: 4.136414051055908\n",
      "Batch: 86, Loss: 4.152472972869873\n",
      "Batch: 87, Loss: 4.153810977935791\n",
      "Batch: 88, Loss: 4.125970363616943\n",
      "Batch: 89, Loss: 4.1094512939453125\n",
      "Batch: 90, Loss: 4.118532657623291\n",
      "Batch: 91, Loss: 4.137163162231445\n",
      "Batch: 92, Loss: 4.139245510101318\n",
      "Batch: 93, Loss: 4.120757102966309\n",
      "Batch: 94, Loss: 4.120924949645996\n",
      "Batch: 95, Loss: 4.100882053375244\n",
      "Batch: 96, Loss: 4.0987396240234375\n",
      "Batch: 97, Loss: 4.1052727699279785\n",
      "Batch: 98, Loss: 4.094323635101318\n",
      "Batch: 99, Loss: 4.0858564376831055\n",
      "Batch: 100, Loss: 4.084300518035889\n",
      "Batch: 101, Loss: 4.0996551513671875\n",
      "Batch: 102, Loss: 4.074913024902344\n",
      "Batch: 103, Loss: 4.0998854637146\n",
      "Batch: 104, Loss: 4.081907272338867\n",
      "Batch: 105, Loss: 4.0890583992004395\n",
      "Batch: 106, Loss: 4.081634998321533\n",
      "Batch: 107, Loss: 4.059865951538086\n",
      "Batch: 108, Loss: 4.077569484710693\n",
      "Batch: 109, Loss: 4.0491814613342285\n",
      "Batch: 110, Loss: 4.043776512145996\n",
      "Batch: 111, Loss: 4.061844348907471\n",
      "Batch: 112, Loss: 4.057991027832031\n",
      "Batch: 113, Loss: 4.049262046813965\n",
      "Batch: 114, Loss: 4.0367913246154785\n",
      "Batch: 115, Loss: 4.029070854187012\n",
      "Batch: 116, Loss: 4.041493892669678\n",
      "Batch: 117, Loss: 4.019955635070801\n",
      "Batch: 118, Loss: 4.030417442321777\n",
      "Batch: 119, Loss: 4.024163722991943\n",
      "Batch: 120, Loss: 4.0020880699157715\n",
      "Batch: 121, Loss: 4.0057053565979\n",
      "Batch: 122, Loss: 4.003355503082275\n",
      "Batch: 123, Loss: 4.009451866149902\n",
      "Batch: 124, Loss: 4.00798225402832\n",
      "Batch: 125, Loss: 3.9846129417419434\n",
      "Batch: 126, Loss: 4.011956214904785\n",
      "Batch: 127, Loss: 3.994791269302368\n",
      "Batch: 128, Loss: 3.9953858852386475\n",
      "Batch: 129, Loss: 3.981114149093628\n",
      "Batch: 130, Loss: 3.9880967140197754\n",
      "Batch: 131, Loss: 3.9944915771484375\n",
      "Batch: 132, Loss: 3.9799458980560303\n",
      "Batch: 133, Loss: 3.9635097980499268\n",
      "Batch: 134, Loss: 3.942009925842285\n",
      "Batch: 135, Loss: 3.9556851387023926\n",
      "Batch: 136, Loss: 3.970594882965088\n",
      "Batch: 137, Loss: 3.95099139213562\n",
      "Batch: 138, Loss: 3.962655782699585\n",
      "Batch: 139, Loss: 3.9492502212524414\n",
      "Batch: 140, Loss: 3.9490933418273926\n",
      "Batch: 141, Loss: 3.9261534214019775\n",
      "Batch: 142, Loss: 3.951941967010498\n",
      "Batch: 143, Loss: 3.933332920074463\n",
      "Batch: 144, Loss: 3.928217887878418\n",
      "Batch: 145, Loss: 3.9146289825439453\n",
      "Batch: 146, Loss: 3.9211037158966064\n",
      "Batch: 147, Loss: 3.9168591499328613\n",
      "Batch: 148, Loss: 3.9140830039978027\n",
      "Batch: 149, Loss: 3.91188907623291\n",
      "Batch: 150, Loss: 3.914278030395508\n",
      "Batch: 151, Loss: 3.9280834197998047\n",
      "Batch: 152, Loss: 3.8865764141082764\n",
      "Batch: 153, Loss: 3.87963604927063\n",
      "Batch: 154, Loss: 3.8964154720306396\n",
      "Batch: 155, Loss: 3.8974971771240234\n",
      "Batch: 156, Loss: 3.901468276977539\n",
      "Batch: 157, Loss: 3.8795814514160156\n",
      "Batch: 158, Loss: 3.8741745948791504\n",
      "Batch: 159, Loss: 3.8810837268829346\n",
      "Batch: 160, Loss: 3.8762667179107666\n",
      "Batch: 161, Loss: 3.8932809829711914\n",
      "Batch: 162, Loss: 3.8775572776794434\n",
      "Batch: 163, Loss: 3.8623945713043213\n",
      "Batch: 164, Loss: 3.869161605834961\n",
      "Batch: 165, Loss: 3.8595573902130127\n",
      "Batch: 166, Loss: 3.852241277694702\n",
      "Batch: 167, Loss: 3.8392434120178223\n",
      "Batch: 168, Loss: 3.8424949645996094\n",
      "Batch: 169, Loss: 3.840435028076172\n",
      "Batch: 170, Loss: 3.8070695400238037\n",
      "Batch: 171, Loss: 3.8469836711883545\n",
      "Batch: 172, Loss: 3.828151226043701\n",
      "Batch: 173, Loss: 3.8328893184661865\n",
      "Batch: 174, Loss: 3.8270535469055176\n",
      "Batch: 175, Loss: 3.823289394378662\n",
      "Batch: 176, Loss: 3.8159377574920654\n",
      "Batch: 177, Loss: 3.8082876205444336\n",
      "Batch: 178, Loss: 3.8068583011627197\n",
      "Batch: 179, Loss: 3.7972233295440674\n",
      "Batch: 180, Loss: 3.8020827770233154\n",
      "Batch: 181, Loss: 3.79384183883667\n",
      "Batch: 182, Loss: 3.806792974472046\n",
      "Batch: 183, Loss: 3.7877213954925537\n",
      "Batch: 184, Loss: 3.788698673248291\n",
      "Batch: 185, Loss: 3.7935984134674072\n",
      "Batch: 186, Loss: 3.7794387340545654\n",
      "Batch: 187, Loss: 3.7818524837493896\n",
      "Batch: 188, Loss: 3.7800815105438232\n",
      "Batch: 189, Loss: 3.769026279449463\n",
      "Batch: 190, Loss: 3.777189016342163\n",
      "Batch: 191, Loss: 3.7606165409088135\n",
      "Batch: 192, Loss: 3.746685266494751\n",
      "Batch: 193, Loss: 3.7665696144104004\n",
      "Batch: 194, Loss: 3.7572314739227295\n",
      "Batch: 195, Loss: 3.7509350776672363\n",
      "Batch: 196, Loss: 3.751152515411377\n",
      "Batch: 197, Loss: 3.7244369983673096\n",
      "Batch: 198, Loss: 3.7471392154693604\n",
      "Batch: 199, Loss: 3.7385406494140625\n",
      "Batch: 200, Loss: 3.736920118331909\n",
      "Batch: 201, Loss: 3.738687038421631\n",
      "Batch: 202, Loss: 3.7389187812805176\n",
      "Batch: 203, Loss: 3.7421791553497314\n",
      "Batch: 204, Loss: 3.744096279144287\n",
      "Batch: 205, Loss: 3.7145466804504395\n",
      "Batch: 206, Loss: 3.737844944000244\n",
      "Batch: 207, Loss: 3.7308502197265625\n",
      "Batch: 208, Loss: 3.7025952339172363\n",
      "Batch: 209, Loss: 3.709479570388794\n",
      "Batch: 210, Loss: 3.70729398727417\n",
      "Batch: 211, Loss: 3.715944766998291\n",
      "Batch: 212, Loss: 3.693007707595825\n",
      "Batch: 213, Loss: 3.6791179180145264\n",
      "Batch: 214, Loss: 3.6855571269989014\n",
      "Batch: 215, Loss: 3.6883912086486816\n",
      "Batch: 216, Loss: 3.6731278896331787\n",
      "Batch: 217, Loss: 3.690437078475952\n",
      "Batch: 218, Loss: 3.6666927337646484\n",
      "Batch: 219, Loss: 3.6816439628601074\n",
      "Batch: 220, Loss: 3.6855885982513428\n",
      "Batch: 221, Loss: 3.6715452671051025\n",
      "Batch: 222, Loss: 3.672271728515625\n",
      "Batch: 223, Loss: 3.6653521060943604\n",
      "Batch: 224, Loss: 3.646552801132202\n",
      "Batch: 225, Loss: 3.6676037311553955\n",
      "Batch: 226, Loss: 3.6535940170288086\n",
      "Batch: 227, Loss: 3.6506357192993164\n",
      "Batch: 228, Loss: 3.6590890884399414\n",
      "Batch: 229, Loss: 3.632763147354126\n",
      "Batch: 230, Loss: 3.67730975151062\n",
      "Batch: 231, Loss: 3.6377131938934326\n",
      "Batch: 232, Loss: 3.6335175037384033\n",
      "Batch: 233, Loss: 3.62385630607605\n",
      "Batch: 234, Loss: 3.611910104751587\n",
      "Epoch: 1, Train loss: 4.058, Val loss: 0.000, Epoch time = 4646.308s\n",
      "Batch: 0, Loss: 3.63505220413208\n",
      "Batch: 1, Loss: 3.6403000354766846\n",
      "Batch: 2, Loss: 3.613372802734375\n",
      "Batch: 3, Loss: 3.6093103885650635\n",
      "Batch: 4, Loss: 3.630621910095215\n",
      "Batch: 5, Loss: 3.610623836517334\n",
      "Batch: 6, Loss: 3.599599599838257\n",
      "Batch: 7, Loss: 3.5980968475341797\n",
      "Batch: 8, Loss: 3.6040663719177246\n",
      "Batch: 9, Loss: 3.5997443199157715\n",
      "Batch: 10, Loss: 3.588900566101074\n",
      "Batch: 11, Loss: 3.593158006668091\n",
      "Batch: 12, Loss: 3.5884366035461426\n",
      "Batch: 13, Loss: 3.617034435272217\n",
      "Batch: 14, Loss: 3.5779216289520264\n",
      "Batch: 15, Loss: 3.5819780826568604\n",
      "Batch: 16, Loss: 3.5779612064361572\n",
      "Batch: 17, Loss: 3.5955660343170166\n",
      "Batch: 18, Loss: 3.5766148567199707\n",
      "Batch: 19, Loss: 3.5819313526153564\n",
      "Batch: 20, Loss: 3.5633163452148438\n",
      "Batch: 21, Loss: 3.5482122898101807\n",
      "Batch: 22, Loss: 3.5575788021087646\n",
      "Batch: 23, Loss: 3.566145658493042\n",
      "Batch: 24, Loss: 3.5544636249542236\n",
      "Batch: 25, Loss: 3.534616708755493\n",
      "Batch: 26, Loss: 3.5257692337036133\n",
      "Batch: 27, Loss: 3.5456511974334717\n",
      "Batch: 28, Loss: 3.5396981239318848\n",
      "Batch: 29, Loss: 3.542097806930542\n",
      "Batch: 30, Loss: 3.5521645545959473\n",
      "Batch: 31, Loss: 3.5502991676330566\n",
      "Batch: 32, Loss: 3.534451484680176\n",
      "Batch: 33, Loss: 3.54150652885437\n",
      "Batch: 34, Loss: 3.497004508972168\n",
      "Batch: 35, Loss: 3.5492606163024902\n",
      "Batch: 36, Loss: 3.5110981464385986\n",
      "Batch: 37, Loss: 3.4924216270446777\n",
      "Batch: 38, Loss: 3.49255108833313\n",
      "Batch: 39, Loss: 3.5030221939086914\n",
      "Batch: 40, Loss: 3.5186805725097656\n",
      "Batch: 41, Loss: 3.4949684143066406\n",
      "Batch: 42, Loss: 3.4758808612823486\n",
      "Batch: 43, Loss: 3.5007169246673584\n",
      "Batch: 44, Loss: 3.4907898902893066\n",
      "Batch: 45, Loss: 3.50089168548584\n",
      "Batch: 46, Loss: 3.488878011703491\n",
      "Batch: 47, Loss: 3.502366065979004\n",
      "Batch: 48, Loss: 3.4663288593292236\n",
      "Batch: 49, Loss: 3.461618661880493\n",
      "Batch: 50, Loss: 3.4643030166625977\n",
      "Batch: 51, Loss: 3.479689836502075\n",
      "Batch: 52, Loss: 3.4584906101226807\n",
      "Batch: 53, Loss: 3.4626646041870117\n",
      "Batch: 54, Loss: 3.441288471221924\n",
      "Batch: 55, Loss: 3.46490740776062\n",
      "Batch: 56, Loss: 3.4538559913635254\n",
      "Batch: 57, Loss: 3.434509038925171\n",
      "Batch: 58, Loss: 3.457402229309082\n",
      "Batch: 59, Loss: 3.4509291648864746\n",
      "Batch: 60, Loss: 3.436493158340454\n",
      "Batch: 61, Loss: 3.44344162940979\n",
      "Batch: 62, Loss: 3.4501123428344727\n",
      "Batch: 63, Loss: 3.430954933166504\n",
      "Batch: 64, Loss: 3.4275708198547363\n",
      "Batch: 65, Loss: 3.42563796043396\n",
      "Batch: 66, Loss: 3.419116973876953\n",
      "Batch: 67, Loss: 3.426814079284668\n",
      "Batch: 68, Loss: 3.389169692993164\n",
      "Batch: 69, Loss: 3.4230639934539795\n",
      "Batch: 70, Loss: 3.4182024002075195\n",
      "Batch: 71, Loss: 3.4133248329162598\n",
      "Batch: 72, Loss: 3.410611867904663\n",
      "Batch: 73, Loss: 3.403615951538086\n",
      "Batch: 74, Loss: 3.4067423343658447\n",
      "Batch: 75, Loss: 3.3640317916870117\n",
      "Batch: 76, Loss: 3.3752310276031494\n",
      "Batch: 77, Loss: 3.4061717987060547\n",
      "Batch: 78, Loss: 3.403301239013672\n",
      "Batch: 79, Loss: 3.385969877243042\n",
      "Batch: 80, Loss: 3.356560468673706\n",
      "Batch: 81, Loss: 3.3869283199310303\n",
      "Batch: 82, Loss: 3.3769514560699463\n",
      "Batch: 83, Loss: 3.3943941593170166\n",
      "Batch: 84, Loss: 3.3923823833465576\n",
      "Batch: 85, Loss: 3.354799270629883\n",
      "Batch: 86, Loss: 3.3796117305755615\n",
      "Batch: 87, Loss: 3.3917667865753174\n",
      "Batch: 88, Loss: 3.3436756134033203\n",
      "Batch: 89, Loss: 3.3277363777160645\n",
      "Batch: 90, Loss: 3.33758282661438\n",
      "Batch: 91, Loss: 3.369422435760498\n",
      "Batch: 92, Loss: 3.3759846687316895\n",
      "Batch: 93, Loss: 3.35355806350708\n",
      "Batch: 94, Loss: 3.350165605545044\n",
      "Batch: 95, Loss: 3.3287181854248047\n",
      "Batch: 96, Loss: 3.3257181644439697\n",
      "Batch: 97, Loss: 3.338730573654175\n",
      "Batch: 98, Loss: 3.323381185531616\n",
      "Batch: 99, Loss: 3.3180081844329834\n",
      "Batch: 100, Loss: 3.3147244453430176\n",
      "Batch: 101, Loss: 3.3351759910583496\n",
      "Batch: 102, Loss: 3.3066318035125732\n",
      "Batch: 103, Loss: 3.344897747039795\n",
      "Batch: 104, Loss: 3.328199863433838\n",
      "Batch: 105, Loss: 3.330515146255493\n",
      "Batch: 106, Loss: 3.3267579078674316\n",
      "Batch: 107, Loss: 3.294370174407959\n",
      "Batch: 108, Loss: 3.3234028816223145\n",
      "Batch: 109, Loss: 3.285433530807495\n",
      "Batch: 110, Loss: 3.280900478363037\n",
      "Batch: 111, Loss: 3.310994863510132\n",
      "Batch: 112, Loss: 3.3117971420288086\n",
      "Batch: 113, Loss: 3.2977325916290283\n",
      "Batch: 114, Loss: 3.2716190814971924\n",
      "Batch: 115, Loss: 3.275242328643799\n",
      "Batch: 116, Loss: 3.296835422515869\n",
      "Batch: 117, Loss: 3.262491464614868\n",
      "Batch: 118, Loss: 3.2887625694274902\n",
      "Batch: 119, Loss: 3.274106025695801\n",
      "Batch: 120, Loss: 3.247283697128296\n",
      "Batch: 121, Loss: 3.2550296783447266\n",
      "Batch: 122, Loss: 3.258715867996216\n",
      "Batch: 123, Loss: 3.265587329864502\n",
      "Batch: 124, Loss: 3.263723373413086\n",
      "Batch: 125, Loss: 3.2354583740234375\n",
      "Batch: 126, Loss: 3.2780368328094482\n",
      "Batch: 127, Loss: 3.2570786476135254\n",
      "Batch: 128, Loss: 3.253145456314087\n",
      "Batch: 129, Loss: 3.2426187992095947\n",
      "Batch: 130, Loss: 3.255985975265503\n",
      "Batch: 131, Loss: 3.263043165206909\n",
      "Batch: 132, Loss: 3.249401807785034\n",
      "Batch: 133, Loss: 3.2282371520996094\n",
      "Batch: 134, Loss: 3.197549343109131\n",
      "Batch: 135, Loss: 3.2191884517669678\n",
      "Batch: 136, Loss: 3.2521233558654785\n",
      "Batch: 137, Loss: 3.214479446411133\n",
      "Batch: 138, Loss: 3.237272024154663\n",
      "Batch: 139, Loss: 3.2198855876922607\n",
      "Batch: 140, Loss: 3.2298686504364014\n",
      "Batch: 141, Loss: 3.1972692012786865\n",
      "Batch: 142, Loss: 3.232269048690796\n",
      "Batch: 143, Loss: 3.211294412612915\n",
      "Batch: 144, Loss: 3.207486629486084\n",
      "Batch: 145, Loss: 3.1892926692962646\n",
      "Batch: 146, Loss: 3.199951171875\n",
      "Batch: 147, Loss: 3.200246810913086\n",
      "Batch: 148, Loss: 3.197706937789917\n",
      "Batch: 149, Loss: 3.1939423084259033\n",
      "Batch: 150, Loss: 3.2054407596588135\n",
      "Batch: 151, Loss: 3.2237908840179443\n",
      "Batch: 152, Loss: 3.1685733795166016\n",
      "Batch: 153, Loss: 3.1652956008911133\n",
      "Batch: 154, Loss: 3.187575578689575\n",
      "Batch: 155, Loss: 3.1910810470581055\n",
      "Batch: 156, Loss: 3.196349620819092\n",
      "Batch: 157, Loss: 3.1681535243988037\n",
      "Batch: 158, Loss: 3.166996717453003\n",
      "Batch: 159, Loss: 3.181332588195801\n",
      "Batch: 160, Loss: 3.174675941467285\n",
      "Batch: 161, Loss: 3.199122428894043\n",
      "Batch: 162, Loss: 3.179511308670044\n",
      "Batch: 163, Loss: 3.1636955738067627\n",
      "Batch: 164, Loss: 3.176879405975342\n",
      "Batch: 165, Loss: 3.1681363582611084\n",
      "Batch: 166, Loss: 3.158513307571411\n",
      "Batch: 167, Loss: 3.138939142227173\n",
      "Batch: 168, Loss: 3.1490092277526855\n",
      "Batch: 169, Loss: 3.1492574214935303\n",
      "Batch: 170, Loss: 3.1059582233428955\n",
      "Batch: 171, Loss: 3.1662299633026123\n",
      "Batch: 172, Loss: 3.1412456035614014\n",
      "Batch: 173, Loss: 3.148228406906128\n",
      "Batch: 174, Loss: 3.136570453643799\n",
      "Batch: 175, Loss: 3.136441946029663\n",
      "Batch: 176, Loss: 3.131901502609253\n",
      "Batch: 177, Loss: 3.1189422607421875\n",
      "Batch: 178, Loss: 3.1227712631225586\n",
      "Batch: 179, Loss: 3.1150248050689697\n",
      "Batch: 180, Loss: 3.1226608753204346\n",
      "Batch: 181, Loss: 3.113083600997925\n",
      "Batch: 182, Loss: 3.138317823410034\n",
      "Batch: 183, Loss: 3.1062111854553223\n",
      "Batch: 184, Loss: 3.1133153438568115\n",
      "Batch: 185, Loss: 3.1268298625946045\n",
      "Batch: 186, Loss: 3.1011407375335693\n",
      "Batch: 187, Loss: 3.1088707447052\n",
      "Batch: 188, Loss: 3.1082510948181152\n",
      "Batch: 189, Loss: 3.0977118015289307\n",
      "Batch: 190, Loss: 3.1072635650634766\n",
      "Batch: 191, Loss: 3.09083890914917\n",
      "Batch: 192, Loss: 3.07861065864563\n",
      "Batch: 193, Loss: 3.10088849067688\n",
      "Batch: 194, Loss: 3.0919110774993896\n",
      "Batch: 195, Loss: 3.0845751762390137\n",
      "Batch: 196, Loss: 3.0887269973754883\n",
      "Batch: 197, Loss: 3.0587985515594482\n",
      "Batch: 198, Loss: 3.085984945297241\n",
      "Batch: 199, Loss: 3.081479072570801\n",
      "Batch: 200, Loss: 3.080806016921997\n",
      "Batch: 201, Loss: 3.0816307067871094\n",
      "Batch: 202, Loss: 3.0887441635131836\n",
      "Batch: 203, Loss: 3.0910727977752686\n",
      "Batch: 204, Loss: 3.0984160900115967\n",
      "Batch: 205, Loss: 3.0675437450408936\n",
      "Batch: 206, Loss: 3.0875582695007324\n",
      "Batch: 207, Loss: 3.081134080886841\n",
      "Batch: 208, Loss: 3.0499322414398193\n",
      "Batch: 209, Loss: 3.0641376972198486\n",
      "Batch: 210, Loss: 3.0632054805755615\n",
      "Batch: 211, Loss: 3.0688302516937256\n",
      "Batch: 212, Loss: 3.049177646636963\n",
      "Batch: 213, Loss: 3.034710168838501\n",
      "Batch: 214, Loss: 3.0419981479644775\n",
      "Batch: 215, Loss: 3.0455515384674072\n",
      "Batch: 216, Loss: 3.0308752059936523\n",
      "Batch: 217, Loss: 3.0515358448028564\n",
      "Batch: 218, Loss: 3.0299415588378906\n",
      "Batch: 219, Loss: 3.039281129837036\n",
      "Batch: 220, Loss: 3.0460286140441895\n",
      "Batch: 221, Loss: 3.033773899078369\n",
      "Batch: 222, Loss: 3.0394484996795654\n",
      "Batch: 223, Loss: 3.031003952026367\n",
      "Batch: 224, Loss: 3.014535427093506\n",
      "Batch: 225, Loss: 3.0337131023406982\n",
      "Batch: 226, Loss: 3.022109031677246\n",
      "Batch: 227, Loss: 3.0211198329925537\n",
      "Batch: 228, Loss: 3.0326952934265137\n",
      "Batch: 229, Loss: 3.0053303241729736\n",
      "Batch: 230, Loss: 3.0539588928222656\n",
      "Batch: 231, Loss: 3.0122334957122803\n",
      "Batch: 232, Loss: 3.008152961730957\n",
      "Batch: 233, Loss: 2.9958808422088623\n",
      "Batch: 234, Loss: 2.988433599472046\n",
      "Epoch: 2, Train loss: 3.294, Val loss: 0.000, Epoch time = 5388.676s\n",
      "Batch: 0, Loss: 3.0150058269500732\n",
      "Batch: 1, Loss: 3.0199837684631348\n",
      "Batch: 2, Loss: 2.992535352706909\n",
      "Batch: 3, Loss: 2.9886856079101562\n",
      "Batch: 4, Loss: 3.010755777359009\n",
      "Batch: 5, Loss: 2.9974963665008545\n",
      "Batch: 6, Loss: 2.983821153640747\n",
      "Batch: 7, Loss: 2.9817886352539062\n",
      "Batch: 8, Loss: 2.989396810531616\n",
      "Batch: 9, Loss: 2.9881927967071533\n",
      "Batch: 10, Loss: 2.9745471477508545\n",
      "Batch: 11, Loss: 2.98213529586792\n",
      "Batch: 12, Loss: 2.9746744632720947\n",
      "Batch: 13, Loss: 3.0085856914520264\n",
      "Batch: 14, Loss: 2.9718995094299316\n",
      "Batch: 15, Loss: 2.9732348918914795\n",
      "Batch: 16, Loss: 2.9739620685577393\n",
      "Batch: 17, Loss: 2.989922285079956\n",
      "Batch: 18, Loss: 2.973252058029175\n",
      "Batch: 19, Loss: 2.9782135486602783\n",
      "Batch: 20, Loss: 2.963697910308838\n",
      "Batch: 21, Loss: 2.9475579261779785\n",
      "Batch: 22, Loss: 2.958777666091919\n",
      "Batch: 23, Loss: 2.9695212841033936\n",
      "Batch: 24, Loss: 2.9623639583587646\n",
      "Batch: 25, Loss: 2.935119152069092\n",
      "Batch: 26, Loss: 2.9330484867095947\n",
      "Batch: 27, Loss: 2.950796365737915\n",
      "Batch: 28, Loss: 2.9471678733825684\n",
      "Batch: 29, Loss: 2.9502792358398438\n",
      "Batch: 30, Loss: 2.959825038909912\n",
      "Batch: 31, Loss: 2.9592905044555664\n",
      "Batch: 32, Loss: 2.9489176273345947\n",
      "Batch: 33, Loss: 2.952575922012329\n",
      "Batch: 34, Loss: 2.9118287563323975\n",
      "Batch: 35, Loss: 2.963233709335327\n",
      "Batch: 36, Loss: 2.9288318157196045\n",
      "Batch: 37, Loss: 2.907845973968506\n",
      "Batch: 38, Loss: 2.9108035564422607\n",
      "Batch: 39, Loss: 2.918440103530884\n",
      "Batch: 40, Loss: 2.936509370803833\n",
      "Batch: 41, Loss: 2.9178450107574463\n",
      "Batch: 42, Loss: 2.899949789047241\n",
      "Batch: 43, Loss: 2.9244747161865234\n",
      "Batch: 44, Loss: 2.9171903133392334\n",
      "Batch: 45, Loss: 2.9231443405151367\n",
      "Batch: 46, Loss: 2.9106838703155518\n",
      "Batch: 47, Loss: 2.9269278049468994\n",
      "Batch: 48, Loss: 2.893385648727417\n",
      "Batch: 49, Loss: 2.8914339542388916\n",
      "Batch: 50, Loss: 2.8941400051116943\n",
      "Batch: 51, Loss: 2.912301540374756\n",
      "Batch: 52, Loss: 2.888930082321167\n",
      "Batch: 53, Loss: 2.8971807956695557\n",
      "Batch: 54, Loss: 2.875370979309082\n",
      "Batch: 55, Loss: 2.900430202484131\n",
      "Batch: 56, Loss: 2.890909433364868\n",
      "Batch: 57, Loss: 2.8737990856170654\n",
      "Batch: 58, Loss: 2.8927693367004395\n",
      "Batch: 59, Loss: 2.8950796127319336\n",
      "Batch: 60, Loss: 2.8764777183532715\n",
      "Batch: 61, Loss: 2.883975028991699\n",
      "Batch: 62, Loss: 2.892453193664551\n",
      "Batch: 63, Loss: 2.878308057785034\n",
      "Batch: 64, Loss: 2.867967128753662\n",
      "Batch: 65, Loss: 2.871580123901367\n",
      "Batch: 66, Loss: 2.8686389923095703\n",
      "Batch: 67, Loss: 2.8771016597747803\n",
      "Batch: 68, Loss: 2.8424036502838135\n",
      "Batch: 69, Loss: 2.8702590465545654\n",
      "Batch: 70, Loss: 2.8740687370300293\n",
      "Batch: 71, Loss: 2.8662354946136475\n",
      "Batch: 72, Loss: 2.864439010620117\n",
      "Batch: 73, Loss: 2.854247570037842\n",
      "Batch: 74, Loss: 2.8629627227783203\n",
      "Batch: 75, Loss: 2.824542284011841\n",
      "Batch: 76, Loss: 2.8362317085266113\n",
      "Batch: 77, Loss: 2.86513352394104\n",
      "Batch: 78, Loss: 2.8625402450561523\n",
      "Batch: 79, Loss: 2.8487436771392822\n",
      "Batch: 80, Loss: 2.820741653442383\n",
      "Batch: 81, Loss: 2.8495399951934814\n",
      "Batch: 82, Loss: 2.841277599334717\n",
      "Batch: 83, Loss: 2.856327772140503\n",
      "Batch: 84, Loss: 2.8579764366149902\n",
      "Batch: 85, Loss: 2.8250768184661865\n",
      "Batch: 86, Loss: 2.8477253913879395\n",
      "Batch: 87, Loss: 2.860602378845215\n",
      "Batch: 88, Loss: 2.814220428466797\n",
      "Batch: 89, Loss: 2.8042702674865723\n",
      "Batch: 90, Loss: 2.8094570636749268\n",
      "Batch: 91, Loss: 2.8413634300231934\n",
      "Batch: 92, Loss: 2.848533868789673\n",
      "Batch: 93, Loss: 2.82863187789917\n",
      "Batch: 94, Loss: 2.832051992416382\n",
      "Batch: 95, Loss: 2.8121085166931152\n",
      "Batch: 96, Loss: 2.808263063430786\n",
      "Batch: 97, Loss: 2.815596580505371\n",
      "Batch: 98, Loss: 2.8075594902038574\n",
      "Batch: 99, Loss: 2.807696580886841\n",
      "Batch: 100, Loss: 2.7995569705963135\n",
      "Batch: 101, Loss: 2.8206868171691895\n",
      "Batch: 102, Loss: 2.796912670135498\n",
      "Batch: 103, Loss: 2.8311827182769775\n",
      "Batch: 104, Loss: 2.817065715789795\n",
      "Batch: 105, Loss: 2.819194793701172\n",
      "Batch: 106, Loss: 2.813701629638672\n",
      "Batch: 107, Loss: 2.788039445877075\n",
      "Batch: 108, Loss: 2.815317153930664\n",
      "Batch: 109, Loss: 2.784348487854004\n",
      "Batch: 110, Loss: 2.781543016433716\n",
      "Batch: 111, Loss: 2.803441047668457\n",
      "Batch: 112, Loss: 2.806814432144165\n",
      "Batch: 113, Loss: 2.795963764190674\n",
      "Batch: 114, Loss: 2.7747442722320557\n",
      "Batch: 115, Loss: 2.7783100605010986\n",
      "Batch: 116, Loss: 2.799861192703247\n",
      "Batch: 117, Loss: 2.7698099613189697\n",
      "Batch: 118, Loss: 2.789776086807251\n",
      "Batch: 119, Loss: 2.779637575149536\n",
      "Batch: 120, Loss: 2.7577831745147705\n",
      "Batch: 121, Loss: 2.766399621963501\n",
      "Batch: 122, Loss: 2.7701516151428223\n",
      "Batch: 123, Loss: 2.7770485877990723\n",
      "Batch: 124, Loss: 2.771693229675293\n",
      "Batch: 125, Loss: 2.750264883041382\n",
      "Batch: 126, Loss: 2.789898157119751\n",
      "Batch: 127, Loss: 2.7709226608276367\n",
      "Batch: 128, Loss: 2.7672524452209473\n",
      "Batch: 129, Loss: 2.7598721981048584\n",
      "Batch: 130, Loss: 2.770933151245117\n",
      "Batch: 131, Loss: 2.779019832611084\n",
      "Batch: 132, Loss: 2.7632272243499756\n",
      "Batch: 133, Loss: 2.74858021736145\n",
      "Batch: 134, Loss: 2.727055788040161\n",
      "Batch: 135, Loss: 2.744713068008423\n",
      "Batch: 136, Loss: 2.770603656768799\n",
      "Batch: 137, Loss: 2.742997169494629\n",
      "Batch: 138, Loss: 2.7583370208740234\n",
      "Batch: 139, Loss: 2.7487502098083496\n",
      "Batch: 140, Loss: 2.758221387863159\n",
      "Batch: 141, Loss: 2.7278473377227783\n",
      "Batch: 142, Loss: 2.7619214057922363\n",
      "Batch: 143, Loss: 2.7401392459869385\n",
      "Batch: 144, Loss: 2.7438690662384033\n",
      "Batch: 145, Loss: 2.7240922451019287\n",
      "Batch: 146, Loss: 2.733757257461548\n",
      "Batch: 147, Loss: 2.7380001544952393\n",
      "Batch: 148, Loss: 2.732664108276367\n",
      "Batch: 149, Loss: 2.733394145965576\n",
      "Batch: 150, Loss: 2.7392289638519287\n",
      "Batch: 151, Loss: 2.7572760581970215\n",
      "Batch: 152, Loss: 2.709911346435547\n",
      "Batch: 153, Loss: 2.7098653316497803\n",
      "Batch: 154, Loss: 2.7311391830444336\n",
      "Batch: 155, Loss: 2.7313618659973145\n",
      "Batch: 156, Loss: 2.737743377685547\n",
      "Batch: 157, Loss: 2.7148313522338867\n",
      "Batch: 158, Loss: 2.714503765106201\n",
      "Batch: 159, Loss: 2.7287681102752686\n",
      "Batch: 160, Loss: 2.7232155799865723\n",
      "Batch: 161, Loss: 2.7459590435028076\n",
      "Batch: 162, Loss: 2.729823350906372\n",
      "Batch: 163, Loss: 2.71618390083313\n",
      "Batch: 164, Loss: 2.7274134159088135\n",
      "Batch: 165, Loss: 2.722651720046997\n",
      "Batch: 166, Loss: 2.7156286239624023\n",
      "Batch: 167, Loss: 2.6965489387512207\n",
      "Batch: 168, Loss: 2.707597255706787\n",
      "Batch: 169, Loss: 2.7054598331451416\n",
      "Batch: 170, Loss: 2.671943187713623\n",
      "Batch: 171, Loss: 2.720708131790161\n",
      "Batch: 172, Loss: 2.7002525329589844\n",
      "Batch: 173, Loss: 2.707371234893799\n",
      "Batch: 174, Loss: 2.7013211250305176\n",
      "Batch: 175, Loss: 2.7008581161499023\n",
      "Batch: 176, Loss: 2.6982691287994385\n",
      "Batch: 177, Loss: 2.6855063438415527\n",
      "Batch: 178, Loss: 2.68841552734375\n",
      "Batch: 179, Loss: 2.6860945224761963\n",
      "Batch: 180, Loss: 2.691206455230713\n",
      "Batch: 181, Loss: 2.684220552444458\n",
      "Batch: 182, Loss: 2.7077391147613525\n",
      "Batch: 183, Loss: 2.680863857269287\n",
      "Batch: 184, Loss: 2.6825411319732666\n",
      "Batch: 185, Loss: 2.6977903842926025\n",
      "Batch: 186, Loss: 2.67329740524292\n",
      "Batch: 187, Loss: 2.686102867126465\n",
      "Batch: 188, Loss: 2.682086944580078\n",
      "Batch: 189, Loss: 2.676363945007324\n",
      "Batch: 190, Loss: 2.679591655731201\n",
      "Batch: 191, Loss: 2.673147678375244\n",
      "Batch: 192, Loss: 2.6616714000701904\n",
      "Batch: 193, Loss: 2.6784615516662598\n",
      "Batch: 194, Loss: 2.672173261642456\n",
      "Batch: 195, Loss: 2.666236639022827\n",
      "Batch: 196, Loss: 2.672416925430298\n",
      "Batch: 197, Loss: 2.6476974487304688\n",
      "Batch: 198, Loss: 2.671041250228882\n",
      "Batch: 199, Loss: 2.661597728729248\n",
      "Batch: 200, Loss: 2.6670658588409424\n",
      "Batch: 201, Loss: 2.6684460639953613\n",
      "Batch: 202, Loss: 2.6764533519744873\n",
      "Batch: 203, Loss: 2.6756787300109863\n",
      "Batch: 204, Loss: 2.6809349060058594\n",
      "Batch: 205, Loss: 2.661294937133789\n",
      "Batch: 206, Loss: 2.6760001182556152\n",
      "Batch: 207, Loss: 2.669806718826294\n",
      "Batch: 208, Loss: 2.6457583904266357\n",
      "Batch: 209, Loss: 2.656403064727783\n",
      "Batch: 210, Loss: 2.6581413745880127\n",
      "Batch: 211, Loss: 2.662952423095703\n",
      "Batch: 212, Loss: 2.6465790271759033\n",
      "Batch: 213, Loss: 2.6361522674560547\n",
      "Batch: 214, Loss: 2.643584966659546\n",
      "Batch: 215, Loss: 2.6441221237182617\n",
      "Batch: 216, Loss: 2.631798505783081\n",
      "Batch: 217, Loss: 2.651092529296875\n",
      "Batch: 218, Loss: 2.6328160762786865\n",
      "Batch: 219, Loss: 2.6388280391693115\n",
      "Batch: 220, Loss: 2.643563747406006\n",
      "Batch: 221, Loss: 2.6387622356414795\n",
      "Batch: 222, Loss: 2.641944169998169\n",
      "Batch: 223, Loss: 2.6343932151794434\n",
      "Batch: 224, Loss: 2.625230312347412\n",
      "Batch: 225, Loss: 2.6362569332122803\n",
      "Batch: 226, Loss: 2.6340227127075195\n",
      "Batch: 227, Loss: 2.626753807067871\n",
      "Batch: 228, Loss: 2.6408886909484863\n",
      "Batch: 229, Loss: 2.6154754161834717\n",
      "Batch: 230, Loss: 2.656191349029541\n",
      "Batch: 231, Loss: 2.6262013912200928\n",
      "Batch: 232, Loss: 2.6228888034820557\n",
      "Batch: 233, Loss: 2.6168723106384277\n",
      "Batch: 234, Loss: 2.6074652671813965\n",
      "Epoch: 3, Train loss: 2.795, Val loss: 0.000, Epoch time = 5837.291s\n",
      "Batch: 0, Loss: 2.6307547092437744\n",
      "Batch: 1, Loss: 2.6342828273773193\n",
      "Batch: 2, Loss: 2.6179122924804688\n",
      "Batch: 3, Loss: 2.609187364578247\n",
      "Batch: 4, Loss: 2.6252565383911133\n",
      "Batch: 5, Loss: 2.6146130561828613\n",
      "Batch: 6, Loss: 2.604611873626709\n",
      "Batch: 7, Loss: 2.6074278354644775\n",
      "Batch: 8, Loss: 2.6127471923828125\n",
      "Batch: 9, Loss: 2.610811471939087\n",
      "Batch: 10, Loss: 2.6006672382354736\n",
      "Batch: 11, Loss: 2.606940984725952\n",
      "Batch: 12, Loss: 2.6023647785186768\n",
      "Batch: 13, Loss: 2.6321332454681396\n",
      "Batch: 14, Loss: 2.5998802185058594\n",
      "Batch: 15, Loss: 2.601118564605713\n",
      "Batch: 16, Loss: 2.6023671627044678\n",
      "Batch: 17, Loss: 2.614013671875\n",
      "Batch: 18, Loss: 2.604149103164673\n",
      "Batch: 19, Loss: 2.6094114780426025\n",
      "Batch: 20, Loss: 2.593899726867676\n",
      "Batch: 21, Loss: 2.585911989212036\n",
      "Batch: 22, Loss: 2.592928647994995\n",
      "Batch: 23, Loss: 2.603400468826294\n",
      "Batch: 24, Loss: 2.5974884033203125\n",
      "Batch: 25, Loss: 2.5749170780181885\n",
      "Batch: 26, Loss: 2.571479558944702\n",
      "Batch: 27, Loss: 2.591238260269165\n",
      "Batch: 28, Loss: 2.5862817764282227\n",
      "Batch: 29, Loss: 2.5902791023254395\n",
      "Batch: 30, Loss: 2.5938518047332764\n",
      "Batch: 31, Loss: 2.601741075515747\n",
      "Batch: 32, Loss: 2.5917539596557617\n",
      "Batch: 33, Loss: 2.5962700843811035\n",
      "Batch: 34, Loss: 2.562032461166382\n",
      "Batch: 35, Loss: 2.6053669452667236\n",
      "Batch: 36, Loss: 2.576242685317993\n",
      "Batch: 37, Loss: 2.5579686164855957\n",
      "Batch: 38, Loss: 2.5599734783172607\n",
      "Batch: 39, Loss: 2.5700201988220215\n",
      "Batch: 40, Loss: 2.5829198360443115\n",
      "Batch: 41, Loss: 2.5694663524627686\n",
      "Batch: 42, Loss: 2.5517995357513428\n",
      "Batch: 43, Loss: 2.570845127105713\n",
      "Batch: 44, Loss: 2.5653076171875\n",
      "Batch: 45, Loss: 2.574767589569092\n",
      "Batch: 46, Loss: 2.5618443489074707\n",
      "Batch: 47, Loss: 2.577620029449463\n",
      "Batch: 48, Loss: 2.551156759262085\n",
      "Batch: 49, Loss: 2.551161050796509\n",
      "Batch: 50, Loss: 2.5523571968078613\n",
      "Batch: 51, Loss: 2.5693304538726807\n",
      "Batch: 52, Loss: 2.546083450317383\n",
      "Batch: 53, Loss: 2.5540401935577393\n",
      "Batch: 54, Loss: 2.5393474102020264\n",
      "Batch: 55, Loss: 2.5629303455352783\n",
      "Batch: 56, Loss: 2.5505874156951904\n",
      "Batch: 57, Loss: 2.536252021789551\n",
      "Batch: 58, Loss: 2.554333209991455\n",
      "Batch: 59, Loss: 2.5555081367492676\n",
      "Batch: 60, Loss: 2.541696548461914\n",
      "Batch: 61, Loss: 2.5473616123199463\n",
      "Batch: 62, Loss: 2.5557587146759033\n",
      "Batch: 63, Loss: 2.546520471572876\n",
      "Batch: 64, Loss: 2.539116144180298\n",
      "Batch: 65, Loss: 2.539105176925659\n",
      "Batch: 66, Loss: 2.537414789199829\n",
      "Batch: 67, Loss: 2.5458197593688965\n",
      "Batch: 68, Loss: 2.517932891845703\n",
      "Batch: 69, Loss: 2.5405726432800293\n",
      "Batch: 70, Loss: 2.5442986488342285\n",
      "Batch: 71, Loss: 2.5375847816467285\n",
      "Batch: 72, Loss: 2.537991762161255\n",
      "Batch: 73, Loss: 2.531008720397949\n",
      "Batch: 74, Loss: 2.538376808166504\n",
      "Batch: 75, Loss: 2.5057451725006104\n",
      "Batch: 76, Loss: 2.515730857849121\n",
      "Batch: 77, Loss: 2.54239821434021\n",
      "Batch: 78, Loss: 2.540051221847534\n",
      "Batch: 79, Loss: 2.525252103805542\n",
      "Batch: 80, Loss: 2.506802797317505\n",
      "Batch: 81, Loss: 2.5271995067596436\n",
      "Batch: 82, Loss: 2.5198655128479004\n",
      "Batch: 83, Loss: 2.5351011753082275\n",
      "Batch: 84, Loss: 2.5395209789276123\n",
      "Batch: 85, Loss: 2.510469913482666\n",
      "Batch: 86, Loss: 2.5276637077331543\n",
      "Batch: 87, Loss: 2.5391628742218018\n",
      "Batch: 88, Loss: 2.500844955444336\n",
      "Batch: 89, Loss: 2.495270013809204\n",
      "Batch: 90, Loss: 2.496988296508789\n",
      "Batch: 91, Loss: 2.5240392684936523\n",
      "Batch: 92, Loss: 2.5333266258239746\n",
      "Batch: 93, Loss: 2.517918586730957\n",
      "Batch: 94, Loss: 2.5237882137298584\n",
      "Batch: 95, Loss: 2.502847671508789\n",
      "Batch: 96, Loss: 2.500965118408203\n",
      "Batch: 97, Loss: 2.5087833404541016\n",
      "Batch: 98, Loss: 2.5014755725860596\n",
      "Batch: 99, Loss: 2.5002498626708984\n",
      "Batch: 100, Loss: 2.497621536254883\n",
      "Batch: 101, Loss: 2.5115966796875\n",
      "Batch: 102, Loss: 2.4901223182678223\n",
      "Batch: 103, Loss: 2.5252013206481934\n",
      "Batch: 104, Loss: 2.5091354846954346\n",
      "Batch: 105, Loss: 2.5138862133026123\n",
      "Batch: 106, Loss: 2.5098485946655273\n",
      "Batch: 107, Loss: 2.4867093563079834\n",
      "Batch: 108, Loss: 2.512669563293457\n",
      "Batch: 109, Loss: 2.4821503162384033\n",
      "Batch: 110, Loss: 2.4843080043792725\n",
      "Batch: 111, Loss: 2.4986917972564697\n",
      "Batch: 112, Loss: 2.5049333572387695\n",
      "Batch: 113, Loss: 2.500420331954956\n",
      "Batch: 114, Loss: 2.478219747543335\n",
      "Batch: 115, Loss: 2.4843201637268066\n",
      "Batch: 116, Loss: 2.50118350982666\n",
      "Batch: 117, Loss: 2.473726987838745\n",
      "Batch: 118, Loss: 2.4929275512695312\n",
      "Batch: 119, Loss: 2.4850077629089355\n",
      "Batch: 120, Loss: 2.467172384262085\n",
      "Batch: 121, Loss: 2.4712700843811035\n",
      "Batch: 122, Loss: 2.4805855751037598\n",
      "Batch: 123, Loss: 2.4825520515441895\n",
      "Batch: 124, Loss: 2.48469614982605\n",
      "Batch: 125, Loss: 2.463707447052002\n",
      "Batch: 126, Loss: 2.495755195617676\n",
      "Batch: 127, Loss: 2.483043670654297\n",
      "Batch: 128, Loss: 2.4782650470733643\n",
      "Batch: 129, Loss: 2.47404408454895\n",
      "Batch: 130, Loss: 2.480400562286377\n",
      "Batch: 131, Loss: 2.488018035888672\n",
      "Batch: 132, Loss: 2.4775993824005127\n",
      "Batch: 133, Loss: 2.467613935470581\n",
      "Batch: 134, Loss: 2.4480605125427246\n",
      "Batch: 135, Loss: 2.4645466804504395\n",
      "Batch: 136, Loss: 2.482368230819702\n",
      "Batch: 137, Loss: 2.4622654914855957\n",
      "Batch: 138, Loss: 2.4730777740478516\n",
      "Batch: 139, Loss: 2.469114303588867\n",
      "Batch: 140, Loss: 2.4733874797821045\n",
      "Batch: 141, Loss: 2.450899600982666\n",
      "Batch: 142, Loss: 2.4802091121673584\n",
      "Batch: 143, Loss: 2.4644219875335693\n",
      "Batch: 144, Loss: 2.467393636703491\n",
      "Batch: 145, Loss: 2.4498322010040283\n",
      "Batch: 146, Loss: 2.4557504653930664\n",
      "Batch: 147, Loss: 2.4610211849212646\n",
      "Batch: 148, Loss: 2.4589695930480957\n",
      "Batch: 149, Loss: 2.4600324630737305\n",
      "Batch: 150, Loss: 2.463688850402832\n",
      "Batch: 151, Loss: 2.4779467582702637\n",
      "Batch: 152, Loss: 2.439659357070923\n",
      "Batch: 153, Loss: 2.4432120323181152\n",
      "Batch: 154, Loss: 2.4567933082580566\n",
      "Batch: 155, Loss: 2.460526704788208\n",
      "Batch: 156, Loss: 2.468168258666992\n",
      "Batch: 157, Loss: 2.44797420501709\n",
      "Batch: 158, Loss: 2.4473063945770264\n",
      "Batch: 159, Loss: 2.457031726837158\n",
      "Batch: 160, Loss: 2.453512191772461\n",
      "Batch: 161, Loss: 2.4739797115325928\n",
      "Batch: 162, Loss: 2.457873582839966\n",
      "Batch: 163, Loss: 2.449335813522339\n",
      "Batch: 164, Loss: 2.461514472961426\n",
      "Batch: 165, Loss: 2.4572925567626953\n",
      "Batch: 166, Loss: 2.451260566711426\n",
      "Batch: 167, Loss: 2.4339146614074707\n",
      "Batch: 168, Loss: 2.4447903633117676\n",
      "Batch: 169, Loss: 2.445070743560791\n",
      "Batch: 170, Loss: 2.4148216247558594\n",
      "Batch: 171, Loss: 2.459902763366699\n",
      "Batch: 172, Loss: 2.4410510063171387\n",
      "Batch: 173, Loss: 2.4464688301086426\n",
      "Batch: 174, Loss: 2.436750650405884\n",
      "Batch: 175, Loss: 2.4405429363250732\n",
      "Batch: 176, Loss: 2.438706636428833\n",
      "Batch: 177, Loss: 2.428968667984009\n",
      "Batch: 178, Loss: 2.428515911102295\n",
      "Batch: 179, Loss: 2.4292211532592773\n",
      "Batch: 180, Loss: 2.4314088821411133\n",
      "Batch: 181, Loss: 2.42928147315979\n",
      "Batch: 182, Loss: 2.4481637477874756\n",
      "Batch: 183, Loss: 2.427935838699341\n",
      "Batch: 184, Loss: 2.4319989681243896\n",
      "Batch: 185, Loss: 2.4434802532196045\n",
      "Batch: 186, Loss: 2.4219419956207275\n",
      "Batch: 187, Loss: 2.4346649646759033\n",
      "Batch: 188, Loss: 2.426197052001953\n",
      "Batch: 189, Loss: 2.4267585277557373\n",
      "Batch: 190, Loss: 2.426950216293335\n",
      "Batch: 191, Loss: 2.4245643615722656\n",
      "Batch: 192, Loss: 2.4148740768432617\n",
      "Batch: 193, Loss: 2.429828643798828\n",
      "Batch: 194, Loss: 2.4236044883728027\n",
      "Batch: 195, Loss: 2.4166805744171143\n",
      "Batch: 196, Loss: 2.4191670417785645\n",
      "Batch: 197, Loss: 2.402398109436035\n",
      "Batch: 198, Loss: 2.423570156097412\n",
      "Batch: 199, Loss: 2.4174046516418457\n",
      "Batch: 200, Loss: 2.4188053607940674\n",
      "Batch: 201, Loss: 2.4216580390930176\n",
      "Batch: 202, Loss: 2.428528308868408\n",
      "Batch: 203, Loss: 2.426734209060669\n",
      "Batch: 204, Loss: 2.43414044380188\n",
      "Batch: 205, Loss: 2.4192895889282227\n",
      "Batch: 206, Loss: 2.4303107261657715\n",
      "Batch: 207, Loss: 2.4279093742370605\n",
      "Batch: 208, Loss: 2.4055025577545166\n",
      "Batch: 209, Loss: 2.4143435955047607\n",
      "Batch: 210, Loss: 2.417891263961792\n",
      "Batch: 211, Loss: 2.420854091644287\n",
      "Batch: 212, Loss: 2.4085941314697266\n",
      "Batch: 213, Loss: 2.3989105224609375\n",
      "Batch: 214, Loss: 2.4027493000030518\n",
      "Batch: 215, Loss: 2.405125856399536\n",
      "Batch: 216, Loss: 2.3949737548828125\n",
      "Batch: 217, Loss: 2.4116578102111816\n",
      "Batch: 218, Loss: 2.396702289581299\n",
      "Batch: 219, Loss: 2.4007232189178467\n",
      "Batch: 220, Loss: 2.4031898975372314\n",
      "Batch: 221, Loss: 2.4045004844665527\n",
      "Batch: 222, Loss: 2.4097883701324463\n",
      "Batch: 223, Loss: 2.398956775665283\n",
      "Batch: 224, Loss: 2.39135479927063\n",
      "Batch: 225, Loss: 2.4023280143737793\n",
      "Batch: 226, Loss: 2.403416156768799\n",
      "Batch: 227, Loss: 2.3971266746520996\n",
      "Batch: 228, Loss: 2.4072089195251465\n",
      "Batch: 229, Loss: 2.387359619140625\n",
      "Batch: 230, Loss: 2.4229319095611572\n",
      "Batch: 231, Loss: 2.3963146209716797\n",
      "Batch: 232, Loss: 2.3948776721954346\n",
      "Batch: 233, Loss: 2.3897383213043213\n",
      "Batch: 234, Loss: 2.379162311553955\n",
      "Epoch: 4, Train loss: 2.496, Val loss: 0.000, Epoch time = 5447.255s\n",
      "Batch: 0, Loss: 2.3983168601989746\n",
      "Batch: 1, Loss: 2.4062139987945557\n",
      "Batch: 2, Loss: 2.391010284423828\n",
      "Batch: 3, Loss: 2.384213924407959\n",
      "Batch: 4, Loss: 2.401615858078003\n",
      "Batch: 5, Loss: 2.3928382396698\n",
      "Batch: 6, Loss: 2.3827385902404785\n",
      "Batch: 7, Loss: 2.384082078933716\n",
      "Batch: 8, Loss: 2.389277935028076\n",
      "Batch: 9, Loss: 2.3876724243164062\n",
      "Batch: 10, Loss: 2.3779029846191406\n",
      "Batch: 11, Loss: 2.385087013244629\n",
      "Batch: 12, Loss: 2.383090019226074\n",
      "Batch: 13, Loss: 2.406803607940674\n",
      "Batch: 14, Loss: 2.3830912113189697\n",
      "Batch: 15, Loss: 2.3799312114715576\n",
      "Batch: 16, Loss: 2.3857054710388184\n",
      "Batch: 17, Loss: 2.395444631576538\n",
      "Batch: 18, Loss: 2.3831231594085693\n",
      "Batch: 19, Loss: 2.3884081840515137\n",
      "Batch: 20, Loss: 2.378916025161743\n",
      "Batch: 21, Loss: 2.371126890182495\n",
      "Batch: 22, Loss: 2.3795018196105957\n",
      "Batch: 23, Loss: 2.3880016803741455\n",
      "Batch: 24, Loss: 2.380443572998047\n",
      "Batch: 25, Loss: 2.3617982864379883\n",
      "Batch: 26, Loss: 2.3582603931427\n",
      "Batch: 27, Loss: 2.373810052871704\n",
      "Batch: 28, Loss: 2.3726789951324463\n",
      "Batch: 29, Loss: 2.375370740890503\n",
      "Batch: 30, Loss: 2.3815195560455322\n",
      "Batch: 31, Loss: 2.3864240646362305\n",
      "Batch: 32, Loss: 2.3783063888549805\n",
      "Batch: 33, Loss: 2.381700277328491\n",
      "Batch: 34, Loss: 2.3501596450805664\n",
      "Batch: 35, Loss: 2.3905723094940186\n",
      "Batch: 36, Loss: 2.3677687644958496\n",
      "Batch: 37, Loss: 2.353233814239502\n",
      "Batch: 38, Loss: 2.354945421218872\n",
      "Batch: 39, Loss: 2.3622708320617676\n",
      "Batch: 40, Loss: 2.37491512298584\n",
      "Batch: 41, Loss: 2.3653571605682373\n",
      "Batch: 42, Loss: 2.348456621170044\n",
      "Batch: 43, Loss: 2.366482973098755\n",
      "Batch: 44, Loss: 2.359018325805664\n",
      "Batch: 45, Loss: 2.368217945098877\n",
      "Batch: 46, Loss: 2.3583364486694336\n",
      "Batch: 47, Loss: 2.3702826499938965\n",
      "Batch: 48, Loss: 2.3494961261749268\n",
      "Batch: 49, Loss: 2.3487017154693604\n",
      "Batch: 50, Loss: 2.347271680831909\n",
      "Batch: 51, Loss: 2.3673248291015625\n",
      "Batch: 52, Loss: 2.3467350006103516\n",
      "Batch: 53, Loss: 2.3533997535705566\n",
      "Batch: 54, Loss: 2.3381829261779785\n",
      "Batch: 55, Loss: 2.3602793216705322\n",
      "Batch: 56, Loss: 2.3525373935699463\n",
      "Batch: 57, Loss: 2.337418556213379\n",
      "Batch: 58, Loss: 2.3522026538848877\n",
      "Batch: 59, Loss: 2.3557491302490234\n",
      "Batch: 60, Loss: 2.3455705642700195\n",
      "Batch: 61, Loss: 2.348257541656494\n",
      "Batch: 62, Loss: 2.3568880558013916\n",
      "Batch: 63, Loss: 2.349930763244629\n",
      "Batch: 64, Loss: 2.3419201374053955\n",
      "Batch: 65, Loss: 2.34623384475708\n",
      "Batch: 66, Loss: 2.341263771057129\n",
      "Batch: 67, Loss: 2.3524019718170166\n",
      "Batch: 68, Loss: 2.3276443481445312\n",
      "Batch: 69, Loss: 2.345489740371704\n",
      "Batch: 70, Loss: 2.3489208221435547\n",
      "Batch: 71, Loss: 2.3468611240386963\n",
      "Batch: 72, Loss: 2.3441789150238037\n",
      "Batch: 73, Loss: 2.339263439178467\n",
      "Batch: 74, Loss: 2.344386577606201\n",
      "Batch: 75, Loss: 2.3188962936401367\n",
      "Batch: 76, Loss: 2.329145908355713\n",
      "Batch: 77, Loss: 2.347378969192505\n",
      "Batch: 78, Loss: 2.346859931945801\n",
      "Batch: 79, Loss: 2.3341636657714844\n",
      "Batch: 80, Loss: 2.3173105716705322\n",
      "Batch: 81, Loss: 2.3401682376861572\n",
      "Batch: 82, Loss: 2.3314576148986816\n",
      "Batch: 83, Loss: 2.346400260925293\n",
      "Batch: 84, Loss: 2.3485538959503174\n",
      "Batch: 85, Loss: 2.3231754302978516\n",
      "Batch: 86, Loss: 2.3394713401794434\n",
      "Batch: 87, Loss: 2.3507561683654785\n",
      "Batch: 88, Loss: 2.315209150314331\n",
      "Batch: 89, Loss: 2.3136377334594727\n",
      "Batch: 90, Loss: 2.3135619163513184\n",
      "Batch: 91, Loss: 2.3341329097747803\n",
      "Batch: 92, Loss: 2.343773603439331\n",
      "Batch: 93, Loss: 2.3356029987335205\n",
      "Batch: 94, Loss: 2.337311029434204\n",
      "Batch: 95, Loss: 2.320326328277588\n",
      "Batch: 96, Loss: 2.3152074813842773\n",
      "Batch: 97, Loss: 2.3223836421966553\n",
      "Batch: 98, Loss: 2.317920207977295\n",
      "Batch: 99, Loss: 2.3180809020996094\n",
      "Batch: 100, Loss: 2.314923048019409\n",
      "Batch: 101, Loss: 2.3281075954437256\n",
      "Batch: 102, Loss: 2.3080244064331055\n",
      "Batch: 103, Loss: 2.3402347564697266\n",
      "Batch: 104, Loss: 2.327791452407837\n",
      "Batch: 105, Loss: 2.3340306282043457\n",
      "Batch: 106, Loss: 2.327971935272217\n",
      "Batch: 107, Loss: 2.31071400642395\n",
      "Batch: 108, Loss: 2.329679489135742\n",
      "Batch: 109, Loss: 2.3075051307678223\n",
      "Batch: 110, Loss: 2.307955265045166\n",
      "Batch: 111, Loss: 2.317422866821289\n",
      "Batch: 112, Loss: 2.3240458965301514\n",
      "Batch: 113, Loss: 2.316453456878662\n",
      "Batch: 114, Loss: 2.3052213191986084\n",
      "Batch: 115, Loss: 2.3085474967956543\n",
      "Batch: 116, Loss: 2.3233816623687744\n",
      "Batch: 117, Loss: 2.302429437637329\n",
      "Batch: 118, Loss: 2.3171212673187256\n",
      "Batch: 119, Loss: 2.3122215270996094\n",
      "Batch: 120, Loss: 2.2978668212890625\n",
      "Batch: 121, Loss: 2.3020050525665283\n",
      "Batch: 122, Loss: 2.3086812496185303\n",
      "Batch: 123, Loss: 2.309082508087158\n",
      "Batch: 124, Loss: 2.3095643520355225\n",
      "Batch: 125, Loss: 2.293488025665283\n",
      "Batch: 126, Loss: 2.3232319355010986\n",
      "Batch: 127, Loss: 2.3092384338378906\n",
      "Batch: 128, Loss: 2.3046979904174805\n",
      "Batch: 129, Loss: 2.3011391162872314\n",
      "Batch: 130, Loss: 2.3083009719848633\n",
      "Batch: 131, Loss: 2.3134169578552246\n",
      "Batch: 132, Loss: 2.3041436672210693\n",
      "Batch: 133, Loss: 2.296659469604492\n",
      "Batch: 134, Loss: 2.2807769775390625\n",
      "Batch: 135, Loss: 2.2951576709747314\n",
      "Batch: 136, Loss: 2.3095221519470215\n",
      "Batch: 137, Loss: 2.2950239181518555\n",
      "Batch: 138, Loss: 2.3042354583740234\n",
      "Batch: 139, Loss: 2.3006536960601807\n",
      "Batch: 140, Loss: 2.3011815547943115\n",
      "Batch: 141, Loss: 2.286158323287964\n",
      "Batch: 142, Loss: 2.311068534851074\n",
      "Batch: 143, Loss: 2.295703411102295\n",
      "Batch: 144, Loss: 2.300215244293213\n",
      "Batch: 145, Loss: 2.288292169570923\n",
      "Batch: 146, Loss: 2.287571907043457\n",
      "Batch: 147, Loss: 2.296523094177246\n",
      "Batch: 148, Loss: 2.295616626739502\n",
      "Batch: 149, Loss: 2.29374623298645\n",
      "Batch: 150, Loss: 2.298456907272339\n",
      "Batch: 151, Loss: 2.3112435340881348\n",
      "Batch: 152, Loss: 2.2810986042022705\n",
      "Batch: 153, Loss: 2.2837882041931152\n",
      "Batch: 154, Loss: 2.294365406036377\n",
      "Batch: 155, Loss: 2.294177532196045\n",
      "Batch: 156, Loss: 2.3033154010772705\n",
      "Batch: 157, Loss: 2.286287307739258\n",
      "Batch: 158, Loss: 2.2841670513153076\n",
      "Batch: 159, Loss: 2.294262409210205\n",
      "Batch: 160, Loss: 2.2949936389923096\n",
      "Batch: 161, Loss: 2.3096485137939453\n",
      "Batch: 162, Loss: 2.29780650138855\n",
      "Batch: 163, Loss: 2.2905185222625732\n",
      "Batch: 164, Loss: 2.2969443798065186\n",
      "Batch: 165, Loss: 2.2980353832244873\n",
      "Batch: 166, Loss: 2.293837547302246\n",
      "Batch: 167, Loss: 2.2755320072174072\n",
      "Batch: 168, Loss: 2.2869656085968018\n",
      "Batch: 169, Loss: 2.28867769241333\n",
      "Batch: 170, Loss: 2.2606282234191895\n",
      "Batch: 171, Loss: 2.2993969917297363\n",
      "Batch: 172, Loss: 2.2834954261779785\n",
      "Batch: 173, Loss: 2.2886009216308594\n",
      "Batch: 174, Loss: 2.2814571857452393\n",
      "Batch: 175, Loss: 2.2854533195495605\n",
      "Batch: 176, Loss: 2.2810983657836914\n",
      "Batch: 177, Loss: 2.27482533454895\n",
      "Batch: 178, Loss: 2.2715635299682617\n",
      "Batch: 179, Loss: 2.2746164798736572\n",
      "Batch: 180, Loss: 2.2779324054718018\n",
      "Batch: 181, Loss: 2.2725889682769775\n",
      "Batch: 182, Loss: 2.289029598236084\n",
      "Batch: 183, Loss: 2.2733495235443115\n",
      "Batch: 184, Loss: 2.277493953704834\n",
      "Batch: 185, Loss: 2.2883808612823486\n",
      "Batch: 186, Loss: 2.2680609226226807\n",
      "Batch: 187, Loss: 2.2812740802764893\n",
      "Batch: 188, Loss: 2.271893262863159\n",
      "Batch: 189, Loss: 2.274716854095459\n",
      "Batch: 190, Loss: 2.2769272327423096\n",
      "Batch: 191, Loss: 2.2770678997039795\n",
      "Batch: 192, Loss: 2.2634973526000977\n",
      "Batch: 193, Loss: 2.2788009643554688\n",
      "Batch: 194, Loss: 2.2738139629364014\n",
      "Batch: 195, Loss: 2.2685024738311768\n",
      "Batch: 196, Loss: 2.2705233097076416\n",
      "Batch: 197, Loss: 2.254406452178955\n",
      "Batch: 198, Loss: 2.2730462551116943\n",
      "Batch: 199, Loss: 2.268005132675171\n",
      "Batch: 200, Loss: 2.268472909927368\n",
      "Batch: 201, Loss: 2.2717173099517822\n",
      "Batch: 202, Loss: 2.27873158454895\n",
      "Batch: 203, Loss: 2.2739219665527344\n",
      "Batch: 204, Loss: 2.2856333255767822\n",
      "Batch: 205, Loss: 2.2729263305664062\n",
      "Batch: 206, Loss: 2.2779440879821777\n",
      "Batch: 207, Loss: 2.2769510746002197\n",
      "Batch: 208, Loss: 2.2598330974578857\n",
      "Batch: 209, Loss: 2.269333600997925\n",
      "Batch: 210, Loss: 2.271355152130127\n",
      "Batch: 211, Loss: 2.2714428901672363\n",
      "Batch: 212, Loss: 2.26303768157959\n",
      "Batch: 213, Loss: 2.254960775375366\n",
      "Batch: 214, Loss: 2.258456230163574\n",
      "Batch: 215, Loss: 2.2625010013580322\n",
      "Batch: 216, Loss: 2.25583815574646\n",
      "Batch: 217, Loss: 2.268052339553833\n",
      "Batch: 218, Loss: 2.2524640560150146\n",
      "Batch: 219, Loss: 2.2576258182525635\n",
      "Batch: 220, Loss: 2.2563934326171875\n",
      "Batch: 221, Loss: 2.2604055404663086\n",
      "Batch: 222, Loss: 2.2651278972625732\n",
      "Batch: 223, Loss: 2.255545139312744\n",
      "Batch: 224, Loss: 2.246246576309204\n",
      "Batch: 225, Loss: 2.2593765258789062\n",
      "Batch: 226, Loss: 2.2603564262390137\n",
      "Batch: 227, Loss: 2.2537903785705566\n",
      "Batch: 228, Loss: 2.2623939514160156\n",
      "Batch: 229, Loss: 2.244079828262329\n",
      "Batch: 230, Loss: 2.2752091884613037\n",
      "Batch: 231, Loss: 2.2532005310058594\n",
      "Batch: 232, Loss: 2.2547497749328613\n",
      "Batch: 233, Loss: 2.2508840560913086\n",
      "Batch: 234, Loss: 2.2441821098327637\n",
      "Epoch: 5, Train loss: 2.317, Val loss: 0.000, Epoch time = 6929.333s\n",
      "Batch: 0, Loss: 2.2566850185394287\n",
      "Batch: 1, Loss: 2.262418746948242\n",
      "Batch: 2, Loss: 2.252168893814087\n",
      "Batch: 3, Loss: 2.242232084274292\n",
      "Batch: 4, Loss: 2.262784957885742\n",
      "Batch: 5, Loss: 2.250530481338501\n",
      "Batch: 6, Loss: 2.2440078258514404\n",
      "Batch: 7, Loss: 2.2432971000671387\n",
      "Batch: 8, Loss: 2.2498300075531006\n",
      "Batch: 9, Loss: 2.2473011016845703\n",
      "Batch: 10, Loss: 2.2388744354248047\n",
      "Batch: 11, Loss: 2.2474796772003174\n",
      "Batch: 12, Loss: 2.2457528114318848\n",
      "Batch: 13, Loss: 2.266732692718506\n",
      "Batch: 14, Loss: 2.2477498054504395\n",
      "Batch: 15, Loss: 2.242985725402832\n",
      "Batch: 16, Loss: 2.249159574508667\n",
      "Batch: 17, Loss: 2.2548458576202393\n",
      "Batch: 18, Loss: 2.24666166305542\n",
      "Batch: 19, Loss: 2.2536046504974365\n",
      "Batch: 20, Loss: 2.2437686920166016\n",
      "Batch: 21, Loss: 2.239314556121826\n",
      "Batch: 22, Loss: 2.243821620941162\n",
      "Batch: 23, Loss: 2.252570867538452\n",
      "Batch: 24, Loss: 2.241959571838379\n",
      "Batch: 25, Loss: 2.227135419845581\n",
      "Batch: 26, Loss: 2.2263355255126953\n",
      "Batch: 27, Loss: 2.242332696914673\n",
      "Batch: 28, Loss: 2.239227056503296\n",
      "Batch: 29, Loss: 2.2418816089630127\n",
      "Batch: 30, Loss: 2.245006561279297\n",
      "Batch: 31, Loss: 2.2508225440979004\n",
      "Batch: 32, Loss: 2.2462265491485596\n",
      "Batch: 33, Loss: 2.246081829071045\n",
      "Batch: 34, Loss: 2.221902847290039\n",
      "Batch: 35, Loss: 2.257240056991577\n",
      "Batch: 36, Loss: 2.2336506843566895\n",
      "Batch: 37, Loss: 2.2209601402282715\n",
      "Batch: 38, Loss: 2.224315881729126\n",
      "Batch: 39, Loss: 2.231656551361084\n",
      "Batch: 40, Loss: 2.2423510551452637\n",
      "Batch: 41, Loss: 2.232097625732422\n",
      "Batch: 42, Loss: 2.2203917503356934\n",
      "Batch: 43, Loss: 2.234633207321167\n",
      "Batch: 44, Loss: 2.228456497192383\n",
      "Batch: 45, Loss: 2.2344882488250732\n",
      "Batch: 46, Loss: 2.2297351360321045\n",
      "Batch: 47, Loss: 2.2401230335235596\n",
      "Batch: 48, Loss: 2.222832441329956\n",
      "Batch: 49, Loss: 2.2244362831115723\n",
      "Batch: 50, Loss: 2.2213807106018066\n",
      "Batch: 51, Loss: 2.2403128147125244\n",
      "Batch: 52, Loss: 2.217283010482788\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      5\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m----> 6\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;66;03m# val_loss = evaluate(transformer)\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[48], line 19\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer)\u001B[0m\n\u001B[0;32m     17\u001B[0m tgt_out \u001B[38;5;241m=\u001B[39m tgt[\u001B[38;5;241m1\u001B[39m:, :]\n\u001B[0;32m     18\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(logits\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, logits\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]), tgt_out\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m---> 19\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     21\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBatch: \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, Loss: \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(i, loss\u001B[38;5;241m.\u001B[39mitem()))\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    512\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    513\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    514\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    519\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    520\u001B[0m     )\n\u001B[1;32m--> 521\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    284\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    286\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 289\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    294\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    295\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    296\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    297\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Workspace\\CharacterBasedNMT\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:769\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    767\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    768\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 769\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    770\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    771\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    772\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    773\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bfebf48e48e40aef",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
